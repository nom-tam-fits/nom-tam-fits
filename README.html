<!DOCTYPE html>


<!--
 | Generated by Apache Maven Doxia Site Renderer 2.0.0-M19 from src/site/markdown/README.md at 2024-10-23
 | Rendered using Apache Maven Fluido Skin 2.0.0-M10
-->
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="generator" content="Apache Maven Doxia Site Renderer 2.0.0-M19" />
    <title>nom-tam-fits</title>
    <link rel="stylesheet" href="./css/apache-maven-fluido-2.0.0-M10.min.css" />
    <link rel="stylesheet" href="./css/site.css" />
    <link rel="stylesheet" href="./css/print.css" media="print" />
    <script src="./js/apache-maven-fluido-2.0.0-M10.min.js"></script>
<meta name="description" content="A full-featured pure Java library for reading, writing, and modifying FITS files." />
			<meta name="robots" content="index,follow" />
			<link rel="shortcut icon" type="image/x-icon" href="/nom-tam-fits/images/favicon.ico" />
			<script async src="https://www.googletagmanager.com/gtag/js?id=G-4TM8JE0R8J"></script>
			<script>
  				window.dataLayer = window.dataLayer || [];
  				function gtag(){dataLayer.push(arguments);}
  				gtag('js', new Date());

			 	gtag('config', 'G-4TM8JE0R8J');
			</script>
    <style>.github-fork-ribbon:before { background-color: gray; }</style>
  </head>
  <body>
    <a class="github-fork-ribbon right-top" href="https://github.com/nom-tam-fits/nom-tam-fits" data-ribbon="Fork me on GitHub">Fork me on GitHub</a>
    <div class="container-fluid container-fluid-top">
      <header>
        <div id="banner">
          <div class="pull-left"><div id="bannerLeft"><h1><a href="https://github.com/nom-tam-fits/nom-tam-fits" class="externalLink"><img class="imageLink" src="images/fits_logo_text.png" /> nom.tam.fits</a></h1></div></div>
          <div class="pull-right"></div>
          <div class="clear"><hr/></div>
        </div>

        <div id="breadcrumbs">
          <ul class="breadcrumb">
        <li id="publishDate">Last Published: 2024-10-23<span class="divider">|</span>
</li>
      <li><a href="index.html">nom.tam.fits</a><span class="divider">/</span></li>

    <li class="active">nom-tam-fits</li>
      <li id="projectVersion" class="pull-right">Version: 1.20.2-SNAPSHOT</li>
          </ul>
        </div>
      </header>
      <div class="row-fluid">
        <header id="leftColumn" class="span2">
          <nav class="well sidebar-nav">
  <ul class="nav nav-list">
   <li class="nav-header">Documentation</li>
    <li><a href="index.html">About</a></li>
    <li class="active"><a>User&apos;s Guide</a></li>
    <li><a href="apidocs/index.html">API Documentation</a></li>
    <li><a href="CHANGELOG.html">Changes</a></li>
    <li><a href="https://github.com/nom-tam-fits/nom-tam-fits/issues" class="externalLink">Issues</a></li>
    <li><a href="https://github.com/nom-tam-fits/nom-tam-fits/discussions" class="externalLink">Discussions</a></li>
    <li><a href="CONTRIBUTING.html">Contributing</a></li>
   <li class="nav-header">Download</li>
    <li><a href="https://github.com/nom-tam-fits/nom-tam-fits/releases" class="externalLink">Releases</a></li>
    <li><a href="maven.html">Maven</a></li>
    <li><a href="https://github.com/nom-tam-fits/nom-tam-fits/" class="externalLink">GitHub</a></li>
   <li class="nav-header">Project Documentation</li>
    <li><a href="project-info.html"><span class="icon-chevron-right"></span>Project Information</a></li>
    <li><a href="project-reports.html"><span class="icon-chevron-right"></span>Project Reports</a></li>
  </ul>
          </nav>
          <div class="well sidebar-nav">
            <div id="poweredBy">
              <div class="clear"></div>
              <div class="clear"></div>
<a href="https://maven.apache.org/" class="builtBy"><img class="builtBy" src="https://maven.apache.org/images/logos/maven-feather.png" /> Maven</a>
            </div>
          </div>
        </header>
        <main id="bodyColumn" class="span10">
<p><img src="https://github.com/nom-tam-fits/nom-tam-fits/actions/workflows/build.yml/badge.svg" alt="Build" />
<img src="https://github.com/nom-tam-fits/nom-tam-fits/actions/workflows/test.yml/badge.svg" alt="Testing" />
<a href="https://oss.sonatype.org/#nexus-search;quick~nom-tam-fits" class="externalLink"><img src="https://github.com/nom-tam-fits/nom-tam-fits/actions/workflows/nexus.yml/badge.svg" alt="Package" /></a>
<a href="https://github.com/nom-tam-fits/nom-tam-fits/actions/workflows/site.yml" class="externalLink"><img src="https://github.com/nom-tam-fits/nom-tam-fits/actions/workflows/site.yml/badge.svg" alt="Project Site" /></a>
<a href="https://codecov.io/gh/nom-tam-fits/nom-tam-fits" class="externalLink"><img src="https://codecov.io/gh/nom-tam-fits/nom-tam-fits/branch/master/graph/badge.svg?token=8rFyA5YzE5" alt="Codecov" /></a>
<a href="https://maven-badges.herokuapp.com/maven-central/gov.nasa.gsfc.heasarc/nom-tam-fits" class="externalLink"><img src="https://maven-badges.herokuapp.com/maven-central/gov.nasa.gsfc.heasarc/nom-tam-fits/badge.svg" alt="Maven Central" /></a></p><section><a id="User.27s_guide"></a>
<h1>User's guide</h1>
<p><strong>nom.tam.fits</strong> is a full-featured, fast, 100% pure Java 8+ library for reading, writing, and modifying
<a href="https://fits.gsfc.nasa.gov/fits_standard.html" class="externalLink">FITS files</a>. The library owes its origins to Tom A. McGlynn (hence the
<em>nom.tam</em> prefix) at NASA Goddard Space Flight Center. Currently it is maintained by
<a href="https://github.com/attipaci" class="externalLink">Attila Kovacs</a> at the Center for Astrophysics | Harvard &amp; Smithsonian.</p>
<p>This document has been updated for 1.20 and/or later 1.x releases.</p><section><a id="Table_of_Contents"></a>
<h2>Table of Contents</h2>
<ul>

<li><a href="#related-links">Related links</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#deprecated-methods">Compatibility with prior releases</a></li>
<li><a href="#reading-fits-files">Reading FITS files</a></li>
<li><a href="#writing-data">Writing FITS data</a></li>
<li><a href="#modifying-existing-files">Modifying existing FITS files</a></li>
<li><a href="#fits-headers">FITS headers</a></li>
<li><a href="#building-tables-from-data">Creating tables</a></li>
<li><a href="#compression-support">Compression support</a></li>
<li><a href="#release-schedule">Release schedule</a></li>
<li><a href="#contribute">How to contribute</a></li>
</ul><hr />
<p></p></section><section><a id="Related_links"></a>
<h2>Related links</h2>
<p>You may find the following links useful:</p>
<ul>

<li><a href="https://nom-tam-fits.github.io/nom-tam-fits/apidocs/index.html" class="externalLink">API documentation</a></li>
<li><a href="https://fits.gsfc.nasa.gov/fits_standard.html" class="externalLink">FITS Standard</a></li>
<li><a href="https://nom-tam-fits.github.io/nom-tam-fits/CHANGELOG.html" class="externalLink">History of changes</a></li>
<li><a href="https://nom-tam-fits.github.io/nom-tam-fits/index.html" class="externalLink">Project site</a> on github.io</li>
<li><a href="https://mvnrepository.com/artifact/gov.nasa.gsfc.heasarc/nom-tam-fits" class="externalLink">Maven Central repository</a></li>
</ul><hr />
<p></p></section><section><a id="Introduction"></a>
<h2>Introduction</h2>
<p>FITS (Flexible Image Transport System) is a binary format of many astronomical datasets and images.</p>
<p>The library requires a level of familiarity with FITS and its common standards and conventions for effective use. For
example, while the library will automatically interpret and populate the mandatory minimum data description in FITS
headers, it will not automatically process many optional standard or conventional header entries. It is mostly up to
the users to extract or complete the description of data to its full extent, for example to include <a href="https://fits.gsfc.nasa.gov/fits_wcs.html" class="externalLink">FITS world
coordinate systems (WCS)</a>, physical units, etc. Users are encouraged to
familiarize themselves with the <a href="https://fits.gsfc.nasa.gov/fits_standard.html" class="externalLink">FITS standard</a> and conventions
described therein to be effective users of this library.</p>
<p>This is an open-source, community maintained, project hosted on GitHub as
<a href="https://github.com/nom-tam-fits/nom-tam-fits" class="externalLink">nom-tam-fits</a>. Further information and documentation, including API
docs, can be found on the <a href="https://nom-tam-fits.github.io/nom-tam-fits/index.html" class="externalLink">project site</a>.</p>
<p></p><section><a id="FITS_data_.28HDU.29_types"></a>
<h3>FITS data (HDU) types</h3>
<p>A FITS file is composed of one or more <em>Header-Data Units</em> (HDUs). Each HDU consists of a <em>header</em>, which describes
the data and possibly contain extra metadata (as key-value pairs) or comments, and a <em>data</em> section.</p>
<p>The current FITS standard (4.0) recognizes the following principal HDU / data types:</p>
<ol style="list-style-type: decimal;">

<li>
<p><strong>Image</strong> can store a regular array of 1-999 dimensions with a type corresponding to Java numerical primitives,
such as a one-dimensional time series of samples (e.g. <code>int[]</code>), or a three-dimensional cube of voxels (e.g.
<code>float[][][]</code>). (Note, that Java supports images up to 255 dimensions only but it's unlikely you'll find that
limiting for your application.)</p></li>
<li>
<p><strong>Binary Table</strong> can store rows and columns of assorted of elements. Each column entry may be either a single
value, or a fixed-sized (multidimensional) array, or else a variable-length 1D arrays of a given type. All Java
primitive numerical types are supported, but also <code>String</code>, <code>Boolean</code> (logical), <code>boolean</code> (bits), and <code>ComplexValue</code>
types.</p></li>
<li>
<p><strong>ASCII Table</strong> (<em>discouraged</em>) is a simpler, less capable table format with support for storing singular
primitive numerical types, and strings only &#x2013; in human-readable format. You should probably use the more flexible
(and more compact) binary tables instead for your application, and reserve use of ASCII tables for reading data that
may still contain these.</p></li>
<li>
<p><strong>Random Groups</strong> (<em>discouraged</em>) can contain a set of images of the same type and dimensions along with a set of
parameters of the same type (for example an <code>int[][]</code> image, along with a set of <code>int</code> parameters). They were never
widely used and the FITS 4.0 standard discourages them going forward, given that binary tables provide far superior
capabilities for storing the same type of data. Support for these type of HDUs is basic, and aimed mainly
at providing a way to access data that was already written in this format.</p></li>
<li>
<p><strong>Foreign File</strong> can encapsulate various other files within the FITS. Foreign file HDUs are a recognized
convention, but not (yet) officially part of the FITS standard. We do not explicitly support foreign file
encapsulation yet, but it is something that we are considering for a future release.</p></li>
</ol>
<p>In addition to the basic HDU types, there are extension of table HDUs that serve specific purposes, such as:</p>
<ul>

<li>
<p><strong>Compressed Images / Tables</strong> are an extension of the binary table HDUs for storing an image or a binary table in
a compressed format, with tiling support to make parts easily accessible from the whole. We provide full support for
compressing and decompressing images and tables, and for accessing specific regions of compressed data stored in this
format.</p></li>
<li>
<p>The <strong>Hierarchical Grouping</strong> convention is an extension of table HDUs (ASCII or binary) for storing information on
the hierarchical relation of HDUs contained within (or external to) the FITS. The hierarchical grouping is a
recognized convention, but not (yet) officially part of the FITS standard. We do not explicitly support this
convention yet, but it is something that we are considering for a future release.</p></li>
</ul><hr />
<p></p></section></section><section><a id="Compatibility_with_prior_releases"></a>
<h2>Compatibility with prior releases</h2>
<p>The current version of the <strong>nom.tam.fits</strong> library requires Java 8 (or later).</p>
<p>We strive to maintain API compatibility with earlier releases of this library, and to an overwhelming extent
we continue to deliver on that. However, in a few corner cases we had no choice but to change the API and/or behavior
slightly to fix bugs, nagging inconsistencies, or non-compliance to the FITS standard. Such changes are generally
rare, and typically affect some of the more obscure features of the library &#x2013; often classes and methods that probably
should never have been exposed to users in the first place. Most typical users (and use cases) of this library will
never see a difference, but some of the more advanced users may find changes that would require some small
modifications to their application in how they use <strong>nom-tam-fits</strong> with recent releases. If you find yourself to be
one of the ones affected, please know that the decision to &#x2018;break&#x2019; previously existing functionality was not taken
lightly, and was done only because it was unavoidable in order to make the library function better overall.</p>
<p>Note, that as of <strong>1.16</strong> we offer only API compatibility to earlier releases, but not binary compatibility. In
practical terms it means that you cannot simply drop-in replace you JAR file, from say version <strong>1.15.2</strong> to
<strong>1.19.0</strong>. Instead, you are expected to (re)compile your application with the JAR version of this library that you
intend to use. This is because some method signatures have changed to use an encompassing argument type, such as
<code>Number</code> instead of the previously separate <code>byte</code>, <code>short</code>, <code>int</code>, <code>long</code>, <code>float</code>, <code>double</code> methods. (These
otherwise harmless API changes improve consistency across numerical types.)</p>
<p>Starting with version <strong>1.16</strong>, we also started deprecating some of the older API, either because methods were
ill-conceived, confusing, or generally unsafe to use; or because they were internals of the library that should never
have been exposed to users in the first place. Rest assured, the deprecations do not cripple the intended
functionality of the library. If anything they make the library less confusing and safer to use. The Javadoc API
documentation mentions alternatives for the methods that were deprecated, as appropriate. And, if nothing else
works, you should still be able to compile your old code with deprecations enabled in the compiler options. Rest
assured, all deprecated methods, no matter how ill-conceived or dodgy they may be, will be supported in all
future releases prior to version <strong>2.0</strong> of the library.</p><hr />
<p></p></section><section><a id="Reading_FITS_files"></a>
<h2>Reading FITS files</h2>
<ul>

<li><a href="#fits-vs-java-bytes">FITS vs Java bytes</a></li>
<li><a href="#deferred-reading">Deferred reading</a></li>
<li><a href="#read-tolerance">Tolerance to standard violations in 3rd party FITS files</a></li>
<li><a href="#reading-images">Reading images</a></li>
<li><a href="#reading-tables">Reading tables</a></li>
</ul>
<p></p><section><a id="FITS_vs_Java_bytes"></a>
<h3>FITS vs Java bytes</h3>
<p>Java bytes are signed, but FITS bytes are not. If any arithmetic processing is to be done on byte-valued data,
users may need to be careful of Java&#x2019;s automated conversion of signed bytes to widened integers. Whereas, a value of
<code>0xFF</code> signifies 255 in FITS, it has a Java value of -1. To preserve the FITS meaning, you may want to upconvert FITS
bytes to Java <code>short</code> values as:</p>

<pre class="prettyprint"><code class="language-java">  short shortValue = (byteValue &amp; 0xFF);
</code></pre>
<p></p></section><section><a id="Deferred_reading"></a>
<h3>Deferred reading</h3>
<p>When FITS data are being read from a non-compressed random accessible input (such as a <code>FitsFile</code>), the <code>read()</code> call
will parse all HDU headers but will typically skip over the data segments (noting their position in the file however).
Only when the user tries to access data from an HDU, will the library load that data from the previously noted file
position. The behavior allows to inspect the contents of a FITS file very quickly even when the file is large, and
reduces the need for IO when only parts of the whole are of interest to the user. Deferred input, however, is not
possible when the input is compressed or if it is uses an stream rather than a random-access <code>FitsFile</code>.</p>
<p>One thing to keep in mind with deferred reading is that you should not close your <code>Fits</code> or its random-accessible
input file before all the required data has been loaded. For example, the following will cause an error:</p>

<pre class="prettyprint"><code class="language-java">  Fits fits = new Fits(&quot;somedata.fits&quot;);
   
  // Scans the FITS, but defers loading data until we need it
  fits.read();
   
  // We close the FITS prematurely.
  fits.close();
   
  // !!!BAD!!! now if  we try to access data
  //           we'll get and exception...
  float[][] image = (float[][]) fits.getHDU(0).getKernel(); 
</code></pre>
<p>In the above, the <code>getKernel()</code> method will try to load the deferred data from the input that we closed just before
it. That's not going to work. The correct order is of course:</p>

<pre class="prettyprint"><code class="language-java">  // Scans the FITS, but defers loading data until we need it
  fits.read();
 
  // Good, the FITS is still open so we can get the deferred data
  float[][] image = (float[][]) fits.getHDU(0).getKernel(); 

  // We close only after we grabbed all the data we needed.
  fits.close();
</code></pre>
<p>As of version <strong>1.18</strong>, all data classes of the library support deferred reading.</p>
<p></p></section><section><a id="Tolerance_to_standard_violations_in_3rd_party_FITS_files."></a>
<h3>Tolerance to standard violations in 3rd party FITS files.</h3>
<p>By default the library will be tolerant to FITS standard violations when parsing 3rd-party FITS files. We believe that
if you use this library to read a FITS produced by other software, you are mainly interested to find out what's inside
it, rather than know if it was written properly. However, problems such as missing padding at the end of the file, or
an unexpected end-of-file before content was fully parsed, will be logged so they can be inspected. Soft violations of
header standards (those that can be overcome with educated guesses) are also tolerated when reading, but logging for
these is not enabled by default (since they may be many, and likely you don't care). You can enable logging standard
violations in 3rd-party headers by <code>Header.setParserWarningsEnabled(true)</code>. You can also enforce stricter compliance
to the standard when reading FITS files via <code>FitsFactory.setAllowHeaderRepairs(false)</code> and
<code>FitsFactory.setAllowTerminalJunk(false)</code>. When violations are not tolerated, appropriate exceptions will be thrown
during reading.</p>
<p></p></section><section><a id="Reading_Images"></a>
<h3>Reading Images</h3>
<ul>

<li><a href="#reading-whole-images">Reading whole images</a></li>
<li><a href="#reading-cutouts">Reading selected parts of images only (cutouts)</a></li>
<li><a href="#streaming-cutouts">Streaming image cutouts</a></li>
<li><a href="#low-level-image-read">Low-level reading of image data</a></li>
</ul>
<p></p><section><a id="Reading_whole_images"></a>
<h4>Reading whole images</h4>
<p>The simplest example of reading an image contained in the first HDU is given below:</p>

<pre class="prettyprint"><code class="language-java">  Fits f = new Fits(&quot;myfile.fits&quot;);
  ImageHDU hdu = (ImageHDU) f.readHDU();
  int[][] image = (int[][]) hdu.getKernel();
</code></pre>
<p>First we create a new instance of <code>Fits</code> with the filename. Then we can get the first HDU using the <code>getHDU()</code> method.</p>
<p>Note the casting into an <code>ImageHDU</code>. When reading FITS data using the nom.tam library the user will often need to cast
the results to the appropriate type. Given that the FITS file may contain many different kinds of data and that Java
provides us with no class that can point to different kinds of primitive arrays other than <code>Object</code>, such explicit
casting is inevitable if you want to use the data from the FITS files.</p></section><section><a id="Converting_images_to_a_different_type"></a>
<h4>Converting images to a different type</h4>
<p>As of version 1.20, the library provides the means for converting images between numerical types. Let's say the FITS
stored the image as integers, but we want them as double-precision values.</p>

<pre class="prettyprint"><code class="language-java">  double[][] darray = (double[][]) hdu.getData().convertTo(double.class).getKernel();
</code></pre>
<p>Things get somewhat interesting at this point, because the FITS standard also allows for the integer representation of
floating-point values, via quantization. The quantization has the following parameters:</p>
<ul>

<li>A scaling factor, i.e. the separation of discrete levels in the floating point data (defaults to 1.0)</li>
<li>an offset, i.e. the floating point value that corresponds to an integer value of 0 (defaults to 0.0).</li>
<li>a blanking value, i.e. the integer that corresponds to NaN (not used by default).</li>
</ul>
<p>The quantization of images is defined by the <code>BSCALE</code>, <code>BZERO</code>, and <code>BLANK</code> keywords accordingly in the FITS header.
Thus, when these keywords are present, the integer to decimal conversion will automatically apply the
integer-to-decimal conversion as:</p>

<pre class="prettyprint"><code class="nohighlight nocode">  &lt;decimal-value&gt; = &lt;scaling&gt; * &lt;integer-value&gt; + &lt;offset&gt;
</code></pre>
<p>or else set NaN values when the integer value matches the designated blanking value. In the absence of the above
header keywords defining quantization, the integer-to-decimal conversion is effectively just a widening conversion.</p>
<p>The reverse conversion, from decimal images to integer images, involves rounding, i.e.:</p>

<pre class="prettyprint"><code class="nohighlight nocode">  &lt;integer-value&gt; = round((&lt;decimal-value&gt; - &lt;offset&gt;) / &lt;scaling&gt;)
</code></pre>
<p>If you want to convert the image without quantization, you may call <code>ImageData.setQuantizer(null)</code> prior to the
conversions, or else use the static low-level <code>ArrayFuncs.convertArray(Object, Class)</code> method instead.</p></section><section><a id="Complex_valued_images"></a>
<h4>Complex valued images</h4>
<p>While the original FITS standard designated images for scalar numerical types only, the current standard (version 4.0)
also specifies a convention to represent complex valued data as images. Complex arrays are recorded as any scalar
numerical type, but with an extra dimension of 2 containing the real and imaginary components. E.g. a 4x3 complex
array can thus be represented as any primitive type array with 4x3x2 dimensions. The convention is that the axis
containing the complex pair of values has is <code>CTYPEn</code> header keyword named as &#x2018;COMPLEX&#x2019;. Note, that complex values can
be recorded as integers also, and use quantization just like decimals.</p>
<p>As of version 1.20, this library recognizes when the convention is used when reading FITS image HDUs. (The library can
only handle the convention if the &#x2018;COMPLEX&#x2019; axis is the first or last image axis &#x2013; which should cover all but some
very pathological use cases.)</p>
<p>However, complex-valued images will continue to read back as their raw storage type (e.g. <code>float[4][3][2]</code>) for back
compatibility with previous releases. But, you can easily check if the data is meant to be complex (or not) and
convert them to complex values in a second step after loading the data. E.g.:</p>

<pre class="prettyprint"><code class="language-java">  // Get the data in the stored data format as some primitive array...
  ImageData data = hdu.getData();
  
  // If the complex array convention was used in the header, we can convert the image data
  // to complex-valued as the second step...
  if (data.isComplexValued()) {
     ComplexValue[][] z = (ComplexValue[][]) data.convertTo(ComplexValue.class).getKernel();
     ...
  }
</code></pre>
<p></p></section><section><a id="Reading_selected_parts_of_an_image_only_.28cutouts.29"></a>
<h4>Reading selected parts of an image only (cutouts)</h4>
<p>Since version <strong>1.18</strong>, it is possible to read select cutouts of large images, including sparse sampling of specific
image regions. When reading image data users may not want to read an entire array especially if the data is very
large. An <code>ImageTiler</code> can be used to read in only a portion of an array. The user can specify a box (or a sequence of
boxes) within the image and extract the desired subsets. <code>ImageTiler</code> can be used for any image. The library will try
to only read the subsets requested if the FITS data is being read from an uncompressed file but in many cases it will
need to read in the entire image before subsetting.</p>
<p>Suppose the image we retrieve above has 2000x2000 pixels, but we only want to see the innermost 100x100 pixels. This
can be achieved with</p>

<pre class="prettyprint"><code class="language-java">  ImageTiler tiler = hdu.getTiler();
  short[] center = (short[]) tiler.getTile(new int[] {950, 950}, new int[] {100, 100});
</code></pre>
<p>The tiler needs to know the corners and size of the tile we want. Note that we can tile an image of any
dimensionality. <code>getTile()</code> returns a one-dimensional array with the flattened 1D image. You can convert it to a 2D
image afterwards using <code>ArrayFuncs.curl()</code>, e.g.:</p>

<pre class="prettyprint"><code class="language-java">  short[][] center2D = (short[][]) ArrayFuncs.curl(center, 100, 100);
</code></pre>
<p>And you can convert to other numerical types, e.g. via one of the <code>ArrayFuncs.convertArray()</code> methods, e.g.:</p>

<pre class="prettyprint"><code class="nohighlight nocode">  double[][] dCenter2D = (double[][]) ArrayFuncs.convertArray(center2D, double.class, hdu.getData().getQuantizer());
</code></pre>
<p></p></section><section><a id="Streaming_image_cutouts"></a>
<h4>Streaming image cutouts</h4>
<p>Since version <strong>1.18</strong> it is also possible to stream cutouts, using the <code>StreamingTileImageData</code> class. The streaming
can be used with any source that implements the <code>RandomAccessFileIO</code> interface, which provides file-like random
access, for example for a resource on the Amazon S3 cloud:</p>

<pre class="prettyprint"><code class="language-java">  import nom.tam.util.RandomAccessFileIO;

  public final class S3RandomAccessFileIO implements RandomAccessFileIO {
      // ...
  }
</code></pre>
<p>Below is an example code sketch for streaming image cutouts from a very large image residing on Amazon S3:</p>

<pre class="prettyprint"><code class="language-java">  Fits source = new Fits(new S3RandomAccessFileIO(...));
  ImageHDU imageHDU = source.getHDU(...);
  
  // Manually set up the header for the cutout image as necessary
  Header cutoutHeader = ...
  
  // Define the image cutout region we want 
  int[] tileStarts, tileLengths, tileSteps;
  ...

  // Create the cutout with the specified parameters
  StreamingTileImageData streamingTileImageData = new StreamingTileImageData(
      cutoutHeader, imageHDU.getTiler(), tileStarts, tileLengths, tileSteps
  );
      
  // Add the cutout region to a new FITS object
  Fits output = new Fits();
  output.addHDU(FitsFactory.hduFactory(cutoutHeader, streamingTileImageData));
      
  // The cutout is processed at write time!
  output.write(outputStream);
</code></pre>
<p>As of version <strong>1.18</strong> it is also possible to stream cutouts from compressed images using the <code>CompressedImageTiler</code>
class. Whereas the <code>asImageHDU()</code> method decompresses the entire image in memory, the <code>CompressedImageTiler</code> will
decompress only the tiles necessary for obtaining the desired cutout. For example, consider writing the cutout from a
compressed image as a regular non-compressed <code>ImageHDU</code>. This can be achieved much the same way as in the above
example, replacing <code>imageHDU.getTiler()</code> with a <code>CompressedImageTiler</code> step, such as:</p>

<pre class="prettyprint"><code class="language-java">  ...
  CompressedImageTiler compressedImageTiler = new CompressedImageTiler(compressedImageHDU);
  StreamingTileImageData streamingTileImageData = new StreamingTileImageData(
      cutoutHeader, compressedImageTiler, corners, lengths, steps
  );
  ...
</code></pre>
<p></p></section><section><a id="Low-level_reading_of_image_data"></a>
<h4>Low-level reading of image data</h4>
<p>Suppose we want to get the average value of a 100,000 x 40,000 pixel image. If the pixels are 32-bit integers, that
would be an 16 GB file. However, we do not need to load the entire image into memory at once. Instead we can analyze
it via bite-sized chunks. For example, we start by finding the beginning of the relevant data segment in the file:</p>

<pre class="prettyprint"><code class="language-java">  Fits fits = new Fits(&quot;bigimg.fits&quot;);
  ImageHDU img = fits.getHDU(0);
  
  // Rewind the stream to the beginning of the data segment
  if (!img.getData().reset()) {
      // Uh-oh...
      throw new IllegalStateException(&quot;Unable to seek to data start&#x201d;);
  }
</code></pre>
<p>The <code>reset()</code> method causes the internal stream to seek to the beginning of the data area. If that&#x2019;s not possible it
returns <code>false</code>. Next, we obtain the input file or stream for reading, query the image size, and set up our
chunk-sized storage (e.g. by image row):</p>

<pre class="prettyprint"><code class="language-java">  // Get the input associated to the FITS
  ArrayDataInput in = fits.getStream();
  
  int[] dims = img.getAxes();      // the image dimensions
  int[] chunk = new int[dims[1]];  // a buffer for a row of data
</code></pre>
<p>Now we can cycle through the image rows (or chunks) and collect the statistics as we go, e.g.:</p>

<pre class="prettyprint"><code class="language-java">  long sum = 0;

  for (int row = 0; row &lt; dims[0]; row++) {
      in.readLArrayFully(chunk); 
      for (int i = 0; i &lt; chunk.length; i++) {
          sum += line[i];
      }
  }
      
  // Return the average value
  return (double) sum / (dims[0] * dims[1]);
</code></pre>
<p></p></section></section><section><a id="Reading_Tables"></a>
<h3>Reading Tables</h3>
<p>The easiest and safest way to access data in tables, is by individual entries. Typically, we start by identifying our
table HDU in the FITS:</p>

<pre class="prettyprint"><code class="language-java">  Fits f = new Fits(&quot;mytable.fits&quot;);

  // Say, our table is the first extension HDU...
  TableHDU hdu = (TableHDU) f.getHDU(1);
</code></pre>
<p>If we are using a random-accessible input (like the file above), we have the option (for binary tables) to load the
entire table into memory first. This may be a good idea for small tables, and/or if we plan to access all the data
contained in the table &#x2013; or not such a good idea if we deal with huge tables from which we need only a selection of
the entries. To load the entire HDU into memory:</p>

<pre class="prettyprint"><code class="language-java">  // This will load the main table and the heap area into memory (if we want to...)
  hdu.getKernel();
</code></pre>
<p>Next, we might want to find which columns store the data we need, using column names if appropriate. (We can of course
rely on hard-coded column indices too when we know we are dealing with tables of known fixed format).</p>

<pre class="prettyprint"><code class="language-java">  // Find column indices by name and check that they exist...
  int colUTC = hdu.findColumn(&quot;UTC&quot;);
  if (colUTC &lt; 0) {
      // uh-oh, there is no such column...
  }
</code></pre>
<p>Now we can loop through the rows of interest and pick out the entries we need. For example, to loop through all table
rows to get only the scalar values from the column named <code>UTC</code> (see above), a phase value in the 4th column (Java
index 3), and a spectrum stored in the fifth column (i.e. Java index 4):</p>

<pre class="prettyprint"><code class="language-java">  // Loop through rows, accessing the relevant column data
  for(int row = 0; row &lt; tab.getNRows(); row++) {
  
      // Retrieve scalar entries with convenient getters... 
      double utc  = tab.getDouble(row, colUTC);
           
      // We can also access by fixed column index...
      ComplexValue phase = (ComplexValue) tab.get(row, 3);
      ComplexValue[] spectrum = (ComplexValue[]) tab.get(row, 4);
      
      // process the data...
      ...
  }
</code></pre>
<p>The old <code>getElement()</code> / <code>setElement()</code> methods supported access as arrays only. While this is still a viable
alternative (though slightly less elegant), we recommend against it going forward. Nevertheless, the equivalent to the
above using this approach would be:</p>

<pre class="prettyprint"><code class="language-java">  // Loop through rows, accessing the relevant column data
  for(int row = 0; row &lt; tab.getNRows(); row++) {
  
      // Retrieve scalar entries by casting the element to the correct array 
      // type, and returning the first (and only) element from that array...
      double utc  = ((double[]) tab.getElement(row, colUTC))[0];
      
      // We can also access by fixed column index...
      float[] phase = ((float[]) tab.getElement(row, 3));
      float[][] spectrum = (float[][]) tab.getElement(row, 4);
      
      // process the data...
      ...
  }
</code></pre>
<p>These older methods (<code>getElement()</code>, <code>getRow()</code> and <code>getColumn()</code>) always return table data as arrays, even
for scalar types, so a single integer entry will be returned as <code>int[1]</code>, a single string as <code>String[1]</code>. Complex
values are stored as <code>float[2]</code> or <code>double[2]</code> depending on  the precision (FITS type <code>C</code> or <code>M</code>). So, a
double-precision FITS complex array of size <code>[5][7]</code> will be returned a <code>double[5][7][2]</code>. Logicals return <code>boolean[]</code>,
which means that while FITS supports <code>null</code> logical values, we don't and these will default to <code>false</code>. (However,
the <code>get()</code> method introduced in version <strong>1.18</strong> will return these as <code>Boolean</code> arrays instead, retaining <code>null</code>
values appropriately!).</p>
<p>Note that for best performance you should access elements in monotonically increasing order when in deferred mode &#x2013; at
least for the rows, but it does not hurt to follow the same principle for columns inside the loops also. This will help
avoid excess buffering that way be required at times for backward jumps.</p>
<p>The library provides methods for accessing entire rows and columns also via the <code>TableData.getRow(int)</code> and
<code>TableData.getColumn(int)</code> or <code>BinaryTable.getColumn(String)</code> methods. However, we recommend against using these going
forward because these methods return data that may be confounding to interpret, with non-trivial data types and/or
dimensions.</p><section><a id="Converting_array_elements"></a>
<h4>Converting array elements</h4>
<p>As of version 1.20, the library also support converting array elements to a different numerical type than the stored
data. Like in the case for images (further above) the integer-decimal conversions will use the columns quantization
parameters if they are defined. Otherwise narrowing conversions of decimal-to-integer types will use simple rounding.
You can convert array elements via the <code>BinaryTable.getArrayElementAs()</code> method. E.g.:</p>

<pre class="prettyprint"><code class="language-java">  BinaryTable tab = ...;

  // Assuming that the column contains 2D numerical entries of some type...
  // Get a table entry as an array of doubles, regardless of its (numerical) storage type...
  double[][] e = tab.getArrayElementAs(1, 3, double.class);
</code></pre>
<p>The quantization of columns is automatically digested based on the <code>TSCALn</code>, <code>TZEROn</code>, and <code>TNULLn</code> keywords in the
table's header, but can be (re)set to a different quantization using the <code>ColumnDesc.setQuantizer()</code> method, if need
be.</p>
<p>Because binary tables already have designated complex-valued column types, the conversions apply only between scalar
numerical types, such as <code>byte.class</code>, <code>short.class</code>, <code>int.class</code>, <code>long.class</code>, <code>float.class</code>, and <code>double.class</code>.</p><hr />
<p></p></section></section></section><section><a id="Writing_FITS_data"></a>
<h2>Writing FITS data</h2>
<ul>

<li><a href="#java-strings-vs-FITS-strings">Java strings vs FITS strings</a></li>
<li><a href="#writing-files">Writing complete FITS files</a></li>
<li><a href="#incremental-writing">Writing one HDU at a time</a></li>
<li><a href="#low-level-writes">Low-level writes</a></li>
</ul>
<p></p><section><a id="Java_strings_vs_FITS_strings"></a>
<h3>Java strings vs FITS strings</h3>
<p>FITS generally represents character strings as byte arrays of ASCII characters, with legal values between <code>0x20</code> and
<code>0x7E</code> (inclusive). The library automatically converts between Java <code>String</code>s and their FITS representations, by the
appropriate narrowing conversion of 16-bit Unicode <code>char</code> to <code>byte</code>. Therefore, you should be careful to avoid using
extended Unicode characters (and also ASCII beyond the <code>0x20</code> &#x2013; <code>0x7E</code> range) in <code>String</code>s, when including these in
FITS.</p>
<p></p></section><section><a id="Writing_complete_FITS_files"></a>
<h3>Writing complete FITS files</h3>
<p>When creating FITS files from data we have at hand, the easiest is to start with a <code>Fits</code> object. We can add to it
image and/or table HDUs we create. When everything is assembled, we write the FITS to a file or stream:</p>

<pre class="prettyprint"><code class="language-java">  Fits fits = new Fits();

  fits.addHDU(...);
  ...
 
  fits.write(&quot;myfits.fits&quot;);
</code></pre>
<p>Images can be added to the FITS at any point. For example, consider a 2D <code>float[][]</code> image we want to add to a FITS:</p>

<pre class="prettyprint"><code class="language-java">  float[][] image ...
  
  ImageHDU imageHDU = Fits.makeHDU(image);
  fits.addHDU(imageHDU);
</code></pre>
<p>The <code>makeHDU()</code> method only populates the essential descriptions of the image in the HDU's header. We may want to
complete that description (e.g. add WCS information, various other data descriptions) to the new HDU's header, e.g.:</p>

<pre class="prettyprint"><code class="language-java">  Header header = imageHDU.getHeader();
  
  header.addValue(Standard.BUNIT, &quot;Jy/beam&quot;);
  ...
</code></pre>
<p>After that we can add further images or table(s), such as binary tables (preferred) or ASCII tables. Once all HDUs
have been assembled this way, we write the FITS as usual:</p>

<pre class="prettyprint"><code class="language-java">  fits.write(&quot;myfits.fits&quot;);
  fits.close();
</code></pre>
<p>An important thing to remember is that while images can be anywhere in the FITS files, tables are extensions, and so,
they may not reside in the first HDU in a file. Thus, if a table is the first HDU we add to a FITS container, it will
be automatically prepended by a dummy primary HDU, and our data will actually be written as the second HDU (Java index
1).</p><section><a id="Binary_versus_ASCII_tables"></a>
<h4>Binary versus ASCII tables</h4>
<p>When writing simple tables it may be possible to write the tables in either binary or ASCII format, provided all
columns are scalar types. By default, the library will create and write binary tables for such data. To create ASCII
tables instead the user should call <code>FitsFactory.setUseAsciiTables(true)</code> first. Given the superiority and
compactness of binary tables, we recommend against using ASCII tables, unless you have to for a compelling reason.</p>
<p></p></section></section><section><a id="Writing_one_HDU_at_a_time"></a>
<h3>Writing one HDU at a time</h3>
<p>Sometimes you do not want to add all your HDUs to a <code>Fits</code> object before writing them out to a file or stream. Maybe
because they use up too much RAM, or you are recording from a live stream and want to add HDUs to the file as they
come in. As of version <strong>1.17</strong> of the library, you can write FITS files one HDU at a time without having to place
them in a <code>Fits</code> container first, or having to worry about the mandatory keywords having been set for primary or
extension HDUs. Or, you can write a <code>Fits</code> object with some number of HDUs, but then keep appending further HDUs
after, worry-free. The <code>FitsFile</code> or <code>FitsOutputStream</code> object will keep track of where things go in the file or
stream, and set the required header keywords for the appended HDUs as appropriate for a primary or extension HDU
automatically.</p>
<p>Here is an example of how to create a FITS file HDU-by-HDU without the need for a <code>Fits</code> object as a holding
container:</p>

<pre class="prettyprint"><code class="language-java">  // Create the file to which to write the HDUs as they come
  FitsFile out = new FitsFile(&quot;my-incremental.fits&quot;, &quot;rw&quot;);
  ...

  // you can append 'hdu' objects to the FITS file (stream) as:
  // The first HDU will be set primary (if possible), and following HDUs will be extensions. 
  hdu.write(out);
  ...

  // When you are all done you can close the FITS file/stream
  out.close(); 
</code></pre>
<p>In the above case the <code>FitsFile</code> output is random accessible, which means you can go back and re-write HDUs (or their
headers) in place later. If you do go all the way back to the head of the file, and re-write the first HDU, you can be
assured that it will contain the necessary header entries for a primary HDU, even if you did not set them yourself.
Easy as pie.</p>
<p>Of course, you can use a <code>FitsOutputStream</code> as opposed to a file as the output also, e.g.:</p>

<pre class="prettyprint"><code class="language-java">  FitsOutputStream out = new FitsOutputStream(new FileOutputStream(&quot;my-incremental.fits&quot;));
  ...
</code></pre>
<p>in which case going back to re-write what was already written before is not an option.</p>
<p></p></section><section><a id="Low-level_writes"></a>
<h3>Low-level writes</h3>
<p>When a large table or image is to be written, the user may wish to stream the write. This is possible but rather
more difficult than in the case of reads.</p>
<p>There are two main issues:</p>
<ol style="list-style-type: decimal;">

<li>
<p>The header for the HDU must written to show the size of the entire file when we are done.
Thus the user may need to modify the header data appropriately.</p></li>
<li>
<p>After writing the data, a valid FITS file may need to be padded to an appropriate length.</p></li>
</ol>
<p>It's not hard to address these requirements, but the user needs some familiarity with the internals of the FITS
representation.</p><section><a id="Images"></a>
<h4>Images</h4>
<p>We can write images one subarray at a time, if we want to. Here is an example of how you could go about it. First,
create storage for the contiguous chunk we want to write at a time. For example, same we want to write a 32-bit
floating-point image  with <code>[nRows][nCols]</code> pixels, and we want to write these one row at a time:</p>
<p>First let's create storage for the chunk:</p>

<pre class="prettyprint"><code class="language-java">  // An array to hold data for a chunk of the image...
  float[] chunk = new float[nCols];
</code></pre>
<p>Next create a header. It's easiest to create it from the chunk, and then just modify the dimensions for the full
image, e.g. as:</p>

<pre class="prettyprint"><code class="language-java">  // Create an image HDU with the row 
  BasicHDU hdu = Fits.makeHDU(row);
  Header header = hdu.getHeader();

  // Override the image dimensions in the header to describe the full image
  ImageData.overrideHeaderAxes(header, nRow, nCol); 
</code></pre>
<p>Next, we can complete the header description adding whatever information we desire. Once complete, we'll write the
image header to the output:</p>

<pre class="prettyprint"><code class="language-java">  // Create a FITS and write to the image to it
  FitsFile out = new FitsFile(&quot;image.fits&quot;, &quot;rw&quot;);
  header.write(out);
</code></pre>
<p>Now, we can start writing the image data, iterating over the rows, populating our chunk data in turn, and writing it
out as we go.</p>

<pre class="prettyprint"><code class="language-java">  // Iterate over the image rows
  for (int i = 0; i &lt; nRows; i++) {
     // fill up the chunk with one row's worth of data
     ...

     // Write the row to the output
     out.writeArray(chunk);
  }
</code></pre>
<p>Finally, add the requisite padding to complete the FITS block of 2880 bytes after the end of the image data:</p>

<pre class="prettyprint"><code class="language-java">  FitsUtil.pad(out, out.position());
  out.close();
</code></pre></section><section><a id="Tables"></a>
<h4>Tables</h4>
<p>We can do something pretty similar for tables <em>so long as we don't have variable length columns</em>, but it requires a
little more work.</p>
<p>First we have to make sure we are not trying to write tables into the primary HDU of a FITS. Tables can only reside in
extensions, and so we might need to create and write a dummy primary HDU to the FITS before we can write the table
itself:</p>

<pre class="prettyprint"><code class="language-java">  FitsFile out = new FitsFile(&quot;table.fits&quot;, &quot;rw&quot;);

  // Binary tables cannot be in the primary HDU of a FITS file
  // So we must add a dummy primary HDU to the FITS first if necessary
  new NullDataHDU().write(out);
</code></pre>
<p>Next, assume we have a binary table that we either read from an input, or else assembled ourselves (see further below
on how to build binary tables):</p>

<pre class="prettyprint"><code class="language-java">  BinaryTable table = ...
</code></pre>
<p>Next, we will need to create an appropriate FITS header for the table:</p>

<pre class="prettyprint"><code class="language-java">  Header header = new Header();
  table.fillHeader(header);
</code></pre>
<p>We can now complete the header description as we see fit, with whatever optional entries. We can also save space for
future additions, e.g. for values we will have only after we start writing the table data itself:</p>

<pre class="prettyprint"><code class="language-java">   // Make space for at least 200 more header lines to be added later
   header.ensureCardSpace(200);
</code></pre>
<p>Now, we can write out the header:</p>

<pre class="prettyprint"><code class="language-java">   header.write(out);
</code></pre>
<p>Next, we can finally write regular table rows (without variable-length entries) in a loop. Assuming that our row is
something like <code>{ { double[1] }, { byte[10] }, { float[256] }, ... }</code>:</p>

<pre class="prettyprint"><code class="language-java">  for (...) {
     // Write data one element at the time into the buffer via the 
     // rowStream. These must match the column structure of the table, 
     // in terms of order, data types, and element counts. 

     out.writeDouble(ra);
     out.write(fixedLengthNameBytes);
     out.witeArray(spectrum);
     ...
  }
</code></pre>
<p>We want to keep count of the rows we write (e.g. <code>nRowsWritten</code>). Once we finish writing the table data, we must add
the requisite padding to complete the FITS block of 2880 bytes after the table data ends.</p>

<pre class="prettyprint"><code class="language-java">  // Add padding to the file to complete the FITS block
  FitsUtil.pad(out, nRowsWritten * table.getRegularRowSize());
</code></pre>
<p>After the table has been thus written to the output, we should make sure that the header has the correct number of
table rows in in <code>NAXIS2</code> entry:</p>

<pre class="prettyprint"><code class="language-java">  header.addValue(Standard.NAXISn.n(2), nRowsWritten);
</code></pre>
<p>We can also complete the header with any other information that became available since the start (using the space we
reserved for additions earlier). Once the header is all in ship-shape, we can re-write in the file at its original
location:</p>

<pre class="prettyprint"><code class="language-java">   // Re-write the header with the new information we added since we began writing 
   // the table data
   header.rewrite();
</code></pre><hr />
<p></p></section></section></section><section><a id="Modifying_existing_FITS_files"></a>
<h2>Modifying existing FITS files</h2>
<p>An existing FITS file can be modified in place in some circumstances. The file must be an uncompressed
(random-accessible) file, with permissions to read and write. The user can then modify elements either by directly
modifying the kernel data object for image data, or by using the <code>setElement</code> or similar methods for tables.</p>
<p>Suppose we have just a couple of specific elements we know we need to change in a given file:</p>

<pre class="prettyprint"><code class="language-java">  Fits f = new Fits(&quot;mod.fits&quot;);
     
  ImageHDU hdu = (ImageHDU) f.getHDU(0);
  int[][] img = (int[][]) hdu.getKernel();
     
  // modify the image as needed...
  img[i][j] = ...
  ...
  
  // write the new data back in the place of the old
  hdu.rewrite();
</code></pre>
<p>Same goes for a table HDU:</p>

<pre class="prettyprint"><code class="language-java">  BinaryTableHDU hdu = (BinaryTableHDU) f.getHDU(1);
  
  // Modify the table as necessary
  hdu.set(3, 0, 3.14159265);
  ...
  
  // Make sure the file contains the changes made above
  hdu.rewrite();
</code></pre>
<p>Note, that in the above table example, the <code>rewrite()</code> call may be superfluous since <code>BinaryTable.set()</code> may be
editing the file in situ if the data has been left in deferred-read mode (random accessible file, without data loaded
to memory). Nevertheless, it is best practice to call <code>rewrite()</code> anyway to ensure that the updates are synced to the
output under all circumstances. And, you should also close the output (e.g. via <code>Fits.close()</code>) after done editing the
FITS file to ensure that any pending file changes are fully flushed to the output.</p>
<p>Defragmenting binary tables allows to reclaim heap space that is no longer used in the heap area. When deleting
variable-length columns, or when replacing entries inside variable-length columns, some or all of the space occupied
by old entries on the heap may become orphaned, needlessly bloating the heap storage. Also, changed entries may be
placed on the heap out of order, which can slow down caching effectiveness for sequential table access. Thus when
modifying tables with variable-length columns, it may be a good idea to defragment the heap before writing in to the
output. For the above example, this would be adding an extra step before <code>rewrite)</code>.</p>

<pre class="prettyprint"><code class="language-java">  ...
  
  // If we changed variable-length data, it may be a good
  // idea to defragment the heap before writing...
  hdu.defragment();

  hdu.rewrite();
</code></pre>
<p>Defragmenting might also be a good idea when building tables with variable-length data column by column (as
opposed to row-by-row).</p>
<p>And, headers can also be updated in place also &#x2013; you don't even need to access the data, which can be left in
deferred state:</p>

<pre class="prettyprint"><code class="language-java">  BasicHDU&lt;?&gt; hdu = f.getHDU(1);
  Header header = hdu.getHeader();
  
  header.addValue(Standard.TELESCOP, &quot;SMA&quot;).comment(&quot;The Submillimeter Array&quot;);
  header.addValue(Standard.DATE-OBS, FitsDate.now());
  ...
  
  header.rewrite();
</code></pre>
<p>Generally rewrites can be made as long as the only change is to the data content, but not to the data size
(and the FITS file meets the criteria mentioned above). An exception will be thrown if the data has been added
or deleted or too many changes have been made to the header. Some additions to the header may be allowed as long as
the header still fits in the same number of FITS blocks (of 2880 bytes) as before. (Hint, you can always reserve
space in headers for later additions using <code>Header.ensureCardSpace(int)</code> prior to writing the header or HDU
originally.)</p><hr />
<p></p></section><section><a id="FITS_headers"></a>
<h2>FITS headers</h2>
<ul>

<li><a href="#what-is-in-a-header">What is in a header</a></li>
<li><a href="#accessing-header-values">Accessing header values</a></li>
<li><a href="#standard-and-conventional-fits-header-keywords">Standard and conventional FITS header keywords</a></li>
<li><a href="#hierarch-style-header-keywords">Hierarchical and long header keywords</a></li>
<li><a href="#long-string-values">Long string values</a></li>
<li><a href="#checksums">Checksums</a></li>
<li><a href="#preallocated-header-space">Preallocated header space</a></li>
<li><a href="#standard-compliance">Standard compliance</a></li>
<li><a href="#migrating-headers">Migrating header data between HDUs</a></li>
</ul>
<p></p><section><a id="What_is_in_a_header"></a>
<h3>What is in a header</h3>
<p>The FITS header consists of a list of 80-byte records at the beginning of each HDU. They contain key/value pairs and
comments and serve three distinct purposes:</p>
<ol style="list-style-type: decimal;">

<li>
<p>First and foremost, the header provides an <em>essential</em> description of the HDU's data segment with a set of
reserved FITS keywords and associated values. These <em>must</em> appear in a specific place and order order in all FITS
headers. The keywords <code>SIMPLE</code> or <code>XTENSION</code>, <code>BITPIX</code>, <code>NAXIS</code>, <code>NAXISn</code>, <code>PCOUNT</code>, <code>GCOUNT</code>, <code>GROUPS</code>, <code>THEAP</code>,
<code>TFIELDS</code>, <code>TTYPEn</code>, <code>TBCOLn</code>, <code>TFORMn</code>, and <code>END</code> form the set of essential keywords. The library automatically
takes care of adding these header entries in the required order, and users of the library should never attempt to
set or modify the essential data description manually.</p></li>
<li>
<p><a href="https://fits.gsfc.nasa.gov/fits_standard.html" class="externalLink">FITS standard</a> also reserves further header keywords to provide
<em>optional</em> standardized descriptions of the data, such as HDU names or versions, physical units, <a href="https://fits.gsfc.nasa.gov/fits_wcs.html" class="externalLink">World Coordinate
Systems (WCS)</a>, column names etc. It is up to the user to familiarize
themselves with the <em>standard</em> keywords and their usage, and use these to describe their data as fully as
appropriate, or to extract information from 3rd party FITS headers.</p></li>
<li>
<p>Finally, the FITS headers may also store a user <em>dictionary</em> of key/value pairs and/or comments. You may store
whatever further information you like (within the constraints of what FITS allows) as long as they stay clear of
the set of reserved FITS keywords mentioned above.</p></li>
</ol>
<p>It is a bit unfortunate that FITS was designed to mix the essential, standard, and user-defined keys in a single
shared space of the same FITS header. It is therefore best practice for all creators of FITS files to:</p>
<ul>

<li>Avoid setting or modifying the essential data description (which could result in corrupted or unreadable FITS
files). Let the library handle these appropriately.</li>
<li>Keep standard (reserved) keywords separated from user-defined keywords in the header if possible. It is
recommended for users to add the standardized header entries first, and then add any/all user-defined entries
after. It is also recommended that users add a comment line (or lines) in-between to clearly denote where the
standard FITS description ends, and where the user dictionary begins after.</li>
<li>Use comment cards to make headers self explanatory and easy for other humans to understand and digest. The header
is also in a sense the self-contained documentation of your FITS data.</li>
</ul>
<p>Note, that originally, header keywords were limited to a maximum of 8 upper-case alphanumeric characters (<code>A</code> to <code>Z</code>
and <code>0</code> to <code>9</code>), plus hyphens (<code>-</code>) and underscores (<code>_</code>), and string values may not exceed 68 characters in length.
However, the <a href="https://fits.gsfc.nasa.gov/registry/hierarch_keyword.html" class="externalLink">HIERARCH keyword convention</a> allows for
longer and/or more extended set of keywords that may utilize the ASCII range from <code>0x21</code> through <code>0x7E</code>, and which
can contain hierarchies. And string values of arbitrary length may be added to headers via the
<a href="https://fits.gsfc.nasa.gov/registry/continue_keyword.html" class="externalLink">CONTINUE long keyword convention</a>, which is now an
integral part of the standard as of FITS version 4.0. See more about these conventions, and their usage within this
library, further below.</p>
<p></p></section><section><a id="Accessing_header_entries"></a>
<h3>Accessing header entries</h3>
<p>There are two basic ways to access data contained in FITS headers: direct (by keyword) or ordered (iterator-based).</p><section><a id="A._Direct_access_header_entries"></a>
<h4>A. Direct access header entries</h4>
<p>You can retrieve keyed values by their associated keyword from the header using the <code>get...Value()</code> methods. To set
values use one of the <code>addValue(...)</code> methods. These methods define a standard dictionary lookup access to key/value
pair stored in the FITS headers.</p>
<p>For example, to find out the telescope or observatory was used to obtain the data you might want to know the value of
the <code>TELESCOP</code> key.</p>

<pre class="prettyprint"><code class="language-java">  Fits f = new Fits(&quot;img.fits&quot;)
  Header header = f.getHDU(0).getHeader();
  String telescope =  header.getStringValue(&quot;TELESCOP&quot;);
</code></pre>
<p>Note, that as of version <strong>1.19</strong> you might want to use one of the <code>Fits.getCompleteHeader(...)</code> methods when
inspecting headers of HDUs stored in the FITS file, since the header returned by these methods would also contain
keywords indirectly inherited from the primary HDU, when the <code>INHERIT</code> keywords is used and set to <code>T</code> (true) in the
header of the specified HDU extension.</p>
<p>You can also use <code>header.getStringValue(Standard.TELESCOPE)</code> instead of the string constant to retrieve the same value
with less chance of a typo spoiling your intent. See more on the use of standard keywords in the section below).</p>
<p>Or if we want to know the right ascension (R.A.) coordinate of the reference position in the image:</p>

<pre class="prettyprint"><code class="language-java">  double ra = header.getDoubleValue(&quot;CRVAL1&quot;); 
</code></pre>
<p>or, equivalently</p>

<pre class="prettyprint"><code class="language-java">  double ra = header.getDoubleValue(WCS.CRVALna.n(1));
</code></pre>
<p>[Note, that the FITS <a href="https://fits.gsfc.nasa.gov/fits_wcs.html" class="externalLink">WCS convention</a> is being used here. For typical 2D
images the reference coordinates are in the pair of keys, <code>CRVAL1</code> and <code>CRVAL2</code> and our example assumes an equatorial
coordinate system.]</p>
<p>To add or change the R.A. coordinate value, you can use:</p>

<pre class="prettyprint"><code class="language-java">  header.addValue(&quot;CRVAL1&quot;, ra, &quot;[deg] R.A. coordinate&quot;);
</code></pre>
<p>or, similarly</p>

<pre class="prettyprint"><code class="language-java">  header.addValue(WCS.CRVALna.n(1), ra);
</code></pre>
<p>The second argument is our new right ascension coordinate (in degrees). The third is a comment field that will also be
written to that header in the space remaining. (When using the standard keyword, the entry is created with the the
standard comment belonging to the keyword, and you may change that by adding <code>.comment(...)</code> if you want it to be
something more specific).</p>
<p>The <code>addValue(...)</code> methods will update existing matching header entries <em>in situ</em> with the newly defined value and
comment, while it will add/insert <em>new</em> header entries at the current <em>mark</em> position. By default, this means that new
entries will be appended at the end of the header, unless you have called <code>Header.findCard(...)</code> earlier to change the
<em>mark</em> position at which new card are added to that immediately before the specified other card, or else you called
<code>Header.seekHead()</code> to add new cards at the start of the (non-essential) header space. Note, that you can always
restore the default behavior of adding new entries at the end by calling <code>Header.seekTail()</code>, if desired. (This may be
a little confusing at first, but the origins of the position marking behavior go a long way back in the history of the
library, and therefore it is here to stay until at least version <strong>2.0</strong>.)</p>
<p>Note, that the <em>mark</em> position also applies to adding comment cards via <code>Header.insertComment()</code>, <code>.insertHistory()</code>,
<code>.insertCommentStyle()</code> and related methods.</p>
<p>Thus, <code>Header.findCard()</code>, <code>.seekHead()</code> and/or <code>.seekTail()</code> methods will allow you to surgically control header order
when adding new cards to headers using the direct access methods.</p>
<p>Table HDUs may contain several standard keywords to describe individual columns, and the <code>TableHDU.setColumnMeta(...)</code>
methods can help you add these optional descriptor for your data while keeping column-specific keywords organized into
header blocks around the mandatory <code>TFORMn</code> keywords. Note, that the <code>.setColumnMeta(...)</code> methods also change the mark
position at which new header entries are added.</p></section><section><a id="B._Iterator-based_access_of_header_values"></a>
<h4>B. Iterator-based access of header values</h4>
<p>For ordered access of header values you can also use the <code>nom.tam.util.Cursor</code> interface to step through header cards
in the order they are stored in the FITS.</p>

<pre class="prettyprint"><code class="language-java">  Cursor&lt;String, HeaderCard&gt; c = header.iterator();
</code></pre>
<p>returns a cursor object that points to the first card of the header. The <code>Cursor.prev()</code> and <code>.next()</code> methods allow
to step through the header, and <code>.add()</code> and <code>.delete()</code> methods can add/remove records at specific locations. The
methods of <code>HeaderCard</code> allow us to manipulate the contents of the current card as desired. Comment and history header
cards can be created and added to the header, e.g. via <code>HeaderCard.createCommentCard()</code> or <code>.createHistoryCard()</code>
respectively.</p>
<p>Note, that the iterator-based approach is the only way to extract comment cards from a header (if you are so inclined),
since dictionary lookup will not work for these &#x2013; as comment cards are by definition not key/value pairs.</p>
<p></p></section></section><section><a id="Standard_and_conventional_FITS_header_keywords"></a>
<h3>Standard and conventional FITS header keywords</h3>
<p>The <a href="https://heasarc.gsfc.nasa.gov/docs/fcg/standard_dict.html" class="externalLink">FITS standard</a> defines a set of reserved keywords.
You can find a collection of these under the <code>nom.tam.fits.header</code> package:</p>
<ul>

<li><code>Standard</code> &#x2013; The core keywords of the FITS standard.</li>
<li><code>WCS</code> &#x2013; Standard FITS Word coordinate system (WCS) keywords</li>
<li><code>DateTime</code> &#x2013; Standard date-time related FITS keywords</li>
<li><code>Compression</code> &#x2013; Standard keywords used for describing compressed data</li>
<li><code>Checksum</code> &#x2013; Standard keywords used for data checksumming</li>
</ul>
<p>In addition to the keywords defined by the FITS standard, the library also recognizes further conventional and
commonly used keyword, which as also collected in the <code>nom.tam.fits.header</code> package:</p>
<ul>

<li><code>HierarchicalGrouping</code> &#x2013; Keywords for the
<a href="https://fits.gsfc.nasa.gov/registry/grouping.html" class="externalLink">Hierarchical Grouping Convention</a></li>
<li><code>NonStandard</code> &#x2013; A few commonly used and recognized keywords that are not strictly part of the FITS standard</li>
<li><code>DataDescription</code> &#x2013; Conventional keywords for describing the data content</li>
<li><code>InstrumentDescription</code> &#x2013; Conventional keywords for describing the instrumentation used for observing</li>
<li><code>ObservationDescription</code> &#x2013; Commonly used keywords that describe the observation</li>
<li><code>ObservationDurationDescription</code> &#x2013; Commonly used keywords for the timing of observations</li>
</ul>
<p>Finally, many organizations (or groups of organizations) have defined their own sets of FITS keywords. Some of
these can be found under the <code>nom.tam.fits-header.extra</code> package, such as:</p>
<ul>

<li><code>CommonExt</code> &#x2013; Commonly used keywords in the amateur astronomy community</li>
<li><code>CXCExt</code> &#x2013; <a href="https://cxc.harvard.edu/contrib/arots/fits/content.txt" class="externalLink">keywords defined for the Chandra X-ray Observatory</a></li>
<li><code>ESOExt</code> &#x2013; keywords specific to the ESO
<a href="https://archive.eso.org/cms/tools-documentation/dicb/ESO-044156_7_DataInterfaceControlDocument.pdf" class="externalLink">DataInterface Control Document</a></li>
<li><code>NOAOExt</code> &#x2013; keywords used by the National Optical Astronomy Observatory (<em>no longer available since the IRAF
project is no longer supported</em>)</li>
<li><code>SBFitsExt</code> &#x2013; <a href="https://diffractionlimited.com/wp-content/uploads/2016/11/sbfitsext_1r0.pdf" class="externalLink">Santa Barbara Instrument Group FITS Extension
(SBFITSEXT)</a>, a.k.a. SBIG keywords.
<ul>

<li><code>MaxImDLExt</code> &#x2013; <a href="https://www.cyanogen.com/help/maximdl/FITS_File_Header_Definitions.htm" class="externalLink">MaxIm DL Astronomy and Scientific Imaging Solutions</a></li>
</ul></li>
<li><code>STScIExt</code> &#x2013; <a href="https://outerspace.stsci.edu/display/MASTDOCS/Required+Metadata" class="externalLink">keywords used by the Space Telescope Science Institute</a></li>
</ul>
<p>You can use the standardized keywords contained in these enums to populate headers or access header values. For
example,</p>

<pre class="prettyprint"><code class="language-java">  hdr.addValue(Standard.INSTRUME, &quot;My super-duper camera&quot;);
  hdr.addValue(InstrumentDescription.FILTER, &quot;Meade #25A Red&quot;);
  ...
</code></pre>
<p>The advantage of using these standardized keywords, as opposed to strings, is that they help avoid keyword typos,
since the compiler (or your IDE) will warn you if the keyword name is not recognized.</p>
<p>Some keywords contain indices that must be specified via the <code>n()</code> method. You must specicify one integer (one-based
index) for each &#x2018;n&#x2019; appearing in the keyword name. For example, to set the value of the <code>WAT9_234</code> keyword to the
string value of <code>&quot;50&quot;</code>:</p>

<pre class="prettyprint"><code class="language-java">  hdr.addValue(NOAOExt.WATn_nnn.n(9, 2, 3, 4), &quot;50&quot;);
</code></pre>
<p>For best practice, try rely on the standard keywords, or those in registered conventions, when possible.</p><section><a id="Keyword_checking"></a>
<h4>Keyword checking</h4>
<p>Another advantage of using the standardized keywords implementing the <code>IFitsHeader</code> interface is that the library can
check (since <strong>1.19</strong>) automatically that (<em>a</em>) the keyword is appropriate for the type of HDU it is used in, and (<em>b</em>)
if the keyword is one of the essential keywords that should be set by the library alone without users tinkering with
them. If the keyword should not be used in the header belonging to a specific type of HDU under the current checking
policy, the library will throw an <code>IllegalArgumentException</code>.</p>
<p>You can use <code>Header.setKeywordChecking()</code> to adjust the type of checking to be applied on a per header instance basis,
or use the static <code>Header.setDefaultKeywordChecking()</code> to change the default policy for all newly created headers.</p>
<p>The <code>Header.KeywordCheck</code> enum defines the following policies that may be used:</p>
<ul>

<li><code>NONE</code> &#x2013; no keyword checking will be applied. You can do whatever you want without consequences. This policy is the
most backward compatible one, since we have not done checking before.</li>
<li><code>DATA_TYPE</code> &#x2013; Checks that the keyword is supported by the data type that the header is meant to describe. This is
the default policy since version <strong>1.19</strong> of the library.</li>
<li><code>STRICT</code> &#x2013; In addition to checking if the keyword is suitable for the data type, the library will also prevent
users from setting essential keywords that really should be handled by the library alone (such as <code>SIMPLE</code> or
<code>XTENSION</code>, <code>BITPIX</code>, <code>NAXIS</code> etc.).</li>
</ul></section><section><a id="Value_checking"></a>
<h4>Value checking</h4>
<p>The standardized keywords that implement the <code>IFitsHeader</code> interface can also specify the type of acceptable values
to use. As of <strong>1.19</strong> we will throw an appropriate exception (<code>IllegalArgumentException</code> or
<code>ValueTypeException</code>, depending on the method) if the user attempt to set a value of unsupported type. For
example trying to set the value of the <code>Standard.TELESCOP</code> keyword (which expects a string value) to a boolean will
throw an exception.</p>
<p>The keyword checking policy can be adjusted via the <code>HeaderCard.setValueCheckingPolicy()</code> method.
<code>HeaderCard.ValueCheck</code> defines the following policies:</p>
<ul>

<li><code>NONE</code> &#x2013; no value type checking will be performed. You can do whatever you want without consequences. This policy
is the most backward compatible one, since we have not done checking before.</li>
<li><code>LOGGING</code> &#x2013; Attempting to set values of the wrong type for <code>IFitsHeader</code> keywords will be allowed but a warning
will be logged each time.</li>
<li><code>EXCEPTION</code> &#x2013;  Attempting to set values of the wrong type for <code>IFitsHeader</code> keywords will throw an appropriate
exception, such as <code>ValueTypeException</code> or <code>IllegalArgumentException</code> depending on the method used. This is the
default policy since version <strong>1.19</strong> of the library.</li>
</ul>
<p></p></section></section><section><a id="Hierarchical_and_long_header_keywords"></a>
<h3>Hierarchical and long header keywords</h3>
<p>The standard FITS header keywords consists of maximum 8 upper case letters (<code>A</code> through <code>Z</code>) or numbers (<code>0</code> through <code>9</code>)
and/or dashes (<code>-</code>) and underscores (<code>_</code>). The
<a href="https://fits.gsfc.nasa.gov/registry/hierarch_keyword.html" class="externalLink">HIERARCH keyword convention</a> allows for storing longer and/or
hierarchical sets of FITS keywords, and can support a somewhat more extended set of ASCII characters (in the range of
<code>0x21</code> to <code>0x7E</code>).  Support for HIERARCH-style keywords is enabled by default as of
version <strong>1.16</strong>. HIERARCH support can be toggled if needed via <code>FitsFactory.setUseHierarch(boolean)</code>. By default,
HIERARCH keywords are converted to upper-case only (<strong>cfitsio</strong> convention), so</p>

<pre class="prettyprint"><code class="language-java">  HeaderCard hc = new HeaderCard(Hierarch.key(&quot;my.lower.case.keyword[!]&quot;), &quot;value&quot;, &quot;comment&quot;);
</code></pre>
<p>will write the header entry to FITS as:</p>

<pre class="prettyprint"><code class="nohighlight nocode">  HIERARCH MY LOWER CASE KEYWORD[!] = 'value' / comment
</code></pre>
<p>You can use <code>FitsFactory.getHierarchFormater().setCaseSensitive(true)</code> to allow the use of lower-case characters, and
hence enable case-sensitive keywords also. After the setting, the same card will be written to FITS as:</p>

<pre class="prettyprint"><code class="nohighlight nocode">  HIERARCH my lower case keyword[!] = 'value' / comment
</code></pre>
<p>You may note a few other properties of HIERARCH keywords as implemented by this library:</p>
<ol style="list-style-type: decimal;">

<li>
<p>The case sensitive setting (above) also determines whether or not HIERARCH keywords are converted to upper-case
upon parsing also. As such, the header entry in last example above may be referred as
<code>HIERARCH.my.lower.case.keyword[!]</code> or as <code>HIERARCH.MY.LOWER.CASE.KEYWORD[!]</code> internally after parsing, depending on
whether case-sensitive mode is enabled or not, respectively.</p></li>
<li>
<p>If <code>FitsFactory</code> has HIERARCH support disabled, any attempt to define a HIERARCH-style long keyword will throw a
<code>HierarchNotEnabledException</code> runtime exception. (However, just <code>HIERARCH</code> by itself will still be allowed as a
standard 8-character FITS keyword on its own).</p></li>
<li>
<p>The convention of the library is to refer to HIERARCH keywords internally as a dot-separated hierarchy, preceded
by <code>HIERARCH.</code>, e.g. <code>HIERARCH.my.keyword</code>. (The static methods of the <code>Hierarch</code> class can make it easier to create
such keywords).</p></li>
<li>
<p>The HIERARCH keywords may contain all printable standard ASCII characters that are allowed in FITS headers (<code>0x20</code>
thru <code>0x7E</code>). As such, we take a liberal reading of the ESO convention, which designated only upper-case letters,
numbers, dashes (<code>-</code>) and underscores (<code>_</code>). If you want to conform to the ESO convention more closely, you should
avoid using characters outside of the set of the original convention.</p></li>
<li>
<p>The library adds a space between the keywords and the <code>=</code> sign, as prescribed by the <strong>cfitsio</strong> convention. The
original ESO convention does not require such a space (but certainly allows for it). We add the extra space to offer
better compatibility with <strong>cfitsio</strong>.</p></li>
<li>
<p>The HIERARCH parsing is tolerant, and does not care about extra space (or spaces) between the hierarchical
components or before <code>=</code>. It also recognizes <code>.</code> as a separator of hierarchy besides the conventional white space.
As such the following may all appear in a FITS header to define the same two-component keyword:</p></li>
</ol>

<pre class="prettyprint"><code class="nohighlight nocode">  HIERARCH MY KEYWORD
</code></pre>

<pre class="prettyprint"><code class="nohighlight nocode">  HIERARCH MY.KEYWORD
</code></pre>

<pre class="prettyprint"><code class="nohighlight nocode">  HIERARCH MY .. KEYWORD
</code></pre>
<p></p></section><section><a id="Long_string_values"></a>
<h3>Long string values</h3>
<p>The standard maximum length for string values in the header is 68 characters. As of FITS 4.0, the <a href="https://fits.gsfc.nasa.gov/registry/continue_keyword.html" class="externalLink">CONTINUE long
string convention</a> is part of the standard. And, as of
version <strong>1.16</strong> of this library, it is supported by default. Support for long strings can be turned off (or on again)
via <code>FitsFactory.setLongStringEnabled(boolean)</code> if necessary. If the settings is disabled, any attempt to set a header
value to a string longer than the space available for it in a single 80-character header record will throw a
<code>LongStringsNotEnabledException</code> runtime exception.</p>
<p></p></section><section><a id="Checksums"></a>
<h3>Checksums</h3>
<p>Checksums can be added to (and updated in) the headers of HDUs, and can be used to check the integrity of the FITS
data after. <code>Fits</code>, <code>BasicHDU</code>, and <code>Data</code> classes provide methods both for setting / updating or for verifying
checksums. The checksums will be calculated directly from the file (as of <strong>1.17</strong>) for all data in deferred read
mode. Thus, it is possible to checksum or verify huge FITS files without having to load large volumes of data into RAM
at any point.</p>
<p>You can set the checksums (<code>CHECKSUM</code> and <code>DATASUM</code> keywords) before you write the FITS or the HDU it to disk:</p>

<pre class="prettyprint"><code class="language-java">  BasicHDU&lt;?&gt; hdu;
         
  // ... prepare the HDU and header ...
   
  hdu.setChecksum();
  hdu.write(...);
</code></pre>
<p>Or you can set checksums for all HDUs in your <code>Fits</code> in one go before writing the entire <code>Fits</code> object out to disk:</p>

<pre class="prettyprint"><code class="language-java">  Fits fits;
  
  // ... Compose the FITS with the HDUs ...
  
  fits.setChecksum();
  fits.write(...);
</code></pre>
<p>Then later, as of version <strong>1.18.1</strong>, you can verify the integrity of FITS files using the stored checksums (or data
sums) just as easily too:</p>

<pre class="prettyprint"><code class="language-java">  try (Fits fits = new Fits(&quot;huge-file.fits&quot;)) {
      fits.verifyIntegrity();
  } catch (FitsIntegrityException e) {
      // Failed integrity check
  } catch (...)
      // some other error...
  }
</code></pre>
<p>The above will calculate checksums for each HDU directly from the file without reading the potentially large data into
memory, and compare HDU checksums and/or data checksums to those stored in the FITS headers. The verification can also
be performed on stream inputs but, unlike for files, data will be invariable loaded into memory (at least temporarily).</p>
<p>You can also verify the integrity of HDUs or their data segments individually, via <code>BasicHDU.verifyIntegrity()</code> or
<code>BasicHDU.verifyDataIntegrity()</code> calls on specific HDUs.</p>
<p>Finally, you might want to update the checksums for a FITS you modify in place:</p>

<pre class="prettyprint"><code class="language-java">  Fits fits = new Fits(&quot;my.fits&quot;);
  
  // We'll modify the fist HDU...
  ImageHDU im = (ImageHDU) fits.readHDU();
  float[][] data = (float[][]) im.getData():
  
  // Offset the data by 1.12
  for (int i = 0; i &lt; data.length; i++) 
      for (int j = 0; i &lt; data[0].length; j++)
          data[i][j] += 1.12;
    
  // Calculate new checksums for the HDU
  im.setChecksum();
  im.rewrite();
</code></pre>
<p>Or, (re)calculate and set checksums for all HDUs in a FITS file, once again leaving deferred data in unloaded state
and computing the checksums for these directly from disk:</p>

<pre class="prettyprint"><code class="language-java">  Fits fits = new Fits(&quot;my.fits&quot;);
  fits.setChecksum();
  fits.rewrite();
</code></pre>
<p>The above will work as expected provided the original FITS already had <code>CHECKSUM</code> and <code>DATASUM</code> keys in the HDUs,
or else the headers had enough unused space for adding these without growing the size of the headers. If any of the
headers or data in the <code>Fits</code> have changed size, the <code>Fits.rewrite()</code> call will throw a <code>FitsException</code> without
modifying any of the records. In such cases You may proceed re-writing a selection of the HDUs, or else write the
<code>Fits</code> to a new file with a different size.</p>
<p></p></section><section><a id="Preallocated_header_space"></a>
<h3>Preallocated header space</h3>
<p>Many FITS files are created by live-recording of data, e.g. from astronomical instruments. As such not all header
values may be defined when one begins writing the data segment of the HDU that follows the header. For example, we do
not know in advance how many rows the binary table will contain, which will depend on when the recording will stop.
Other metadata may simply not be available until a later time. For this reason version 4.0 of the FITS standard has
specified preallocating header space as some number of blank header records between the last defined header entry and
the <code>END</code> keyword.</p>
<p>As of version <strong>1.16</strong>, this library supports preallocated header space via <code>Header.ensureCardSpace(int)</code>, which can
be used to ensure that the header can contain <em>at least</em> the specified number of 80-character records when written to
the output. (In reality it may accommodate somewhat more than that because of the required padding to multiples of
2880 bytes or 36 records &#x2013; and you can use <code>Header.getMinimumSize()</code> to find out just how many bytes are
reserved / used by any header object at any point).</p>
<p>Once the space has been reserved, the header can be written to the output, and one may begin recording data after it.
The header can be completed later, up to the number of additional cards specified (and sometimes beyond), and the
updated header can be rewritten place at a later time with the additional entries.</p>
<p>For example,</p>

<pre class="prettyprint"><code class="language-java">  FitsFile out = new FitsFile(&quot;mydata.fits&quot;, &quot;rw&quot;);
  
  Header h = new Header();
  
  // We want to keep room for 200 80-character records in total
  // to be filled later
  h.ensureCardSpace(200);

  // We can now write the header, knowing we can fill it with up to
  // 200 records in total at a later point
  h.write(out);
</code></pre>
<p>Now you can proceed to recording the data, such as a binary table row-by-row. Once you are done with it, you can go
back and make edits to the header, adding more header cards, in the space you reserved earlier, and rewrite the
header in front of the data without issues:</p>

<pre class="prettyprint"><code class="language-java">  // Once the data has been recorded we can proceed to fill in 
  // the additional header values, such as the end time of observation
  h.addValue(&quot;DATE-END&quot;, FitsDate.getFitsDateString(), &quot;end of observation&quot;);
  
  // And we can re-write the header in place
  h.rewrite();
</code></pre>
<p>Preallocated header space is also preserved when reading the data in. When parsing headers trailing blank header
records (before the <code>END</code> key) are counted as reserved card space. (Internal blank cards, between two regular keyword
entries, are however preserved as blank comment cards and their space will not be reusable unless these cards are
explicitly removed first). After reading a header with preallocated space, the user can add at least as many new cards
into that header as trailing blank records were found, and still call <code>rewrite()</code> on that header without any problems.</p>
<p></p></section><section><a id="Standard_compliance"></a>
<h3>Standard compliance</h3>
<p>As of version <strong>1.16</strong>, the library offers a two-pronged approach to ensure header compliance to the
<a href="#https://fits.gsfc.nasa.gov/fits_standard.html" class="externalLink">FITS standard</a>.</p>
<ul>

<li>
<p>First, we fully enforce the standards when creating FITS headers using this library, and we do it in a way that is
compliant with earlier FITS specifications (prior to 4.0) also. We will prevent the creation of non-standard header
entries (cards) by throwing appropriate runtime exceptions (such as <code>IllegalArgumentException</code>, <code>LongValueException</code>,
<code>LongStringsNotEnabledException</code>, <code>HierarchNotEnabledException</code>) as soon as one attempts to set a header component
that is not supported by FITS or by the set of standards selected in the current <code>FitsFactory</code> settings.</p></li>
<li>
<p>Second, we offer the choice between tolerant and strict interpretation of 3rd-party FITS headers when parsing these.
In tolerant mode (default), the parser will do its best to overcome standard violations as much as possible, such that
the header can be parsed as fully as possible, even if some entries may have malformed content. The user may enable <code>Header.setParserWarningsEnabled(true)</code> to log each violation detected by the parser as warnings, so these can be
inspected if the user cares to know. Stricter parsing can be enabled by <code>FitsFactory.setAllowHeaderRepairs(false)</code>.
In this mode, the parser will throw an exception when it encounters a severely corrupted header entry, such as a
string value with no closing quote (<code>UnclosedQuoteException</code>) or a complex value without a closing bracket
(<code>IllegalArgumentException</code>). Lesser violations can still be logged, the same way as in tolerant mode.</p></li>
</ul>
<p>Additionally, we provide <code>HeaderCard.sanitize(String)</code> method that the user can call to ensure that a Java <code>String</code>
can be used in FITS headers. The method will replace illegal FITS characters (outside of the range of <code>0x20</code> thru
<code>0x7E</code>) with <code>?</code>.</p>
<p></p></section></section><section><a id="Migrating_header_data_between_HDUs"></a>
<h2>Migrating header data between HDUs</h2>
<p>Sometimes we want to create a new HDU based on an existing HDU, such as a cropped image, or a table segment, in which
we want to reuse much of the information contained in the original header. The best way to go about it is via the
following steps:</p>
<ol style="list-style-type: decimal;">

<li>
<p>Start by creating the new HDU from the data it will hold. It ensures that the new HDU will have the correct
essential data description (type and size) in its header.</p></li>
<li>
<p>Merge distinct (non-conflicting) header entries from the original HDU into the header of the new HDU, using the
<code>Header.mergeDistinct(Header source)</code> method. It will migrate the header entries from the original HDU to the new one,
without overriding the proper essential data description.</p></li>
<li>
<p>Update the header entries as necessary, such as <a href="https://fits.gsfc.nasa.gov/fits_wcs.html" class="externalLink">WCS</a>, in the new HDU.
Pay attention to removing obsoleted entries also, such as descriptions of table columns that no longer exist in the
new data.</p></li>
<li>
<p>If the header contains checksums, make sure you update these before writing the header or HDU to an output.</p></li>
</ol>
<p>For example:</p>

<pre class="prettyprint"><code class="language-java">  // Some image HDU whose header we want to reuse for another...
  ImageHDU origHDU = ...
  
  // 1. create the new image HDU with the new data
  float[][] newImage = ...
  ImageHDU newHDU = ImageData.from(newImage).toHDU();
  
  // 2. copy over non-conflicting header entries from the original
  Header newHeader = newHDU.getHeader();
  newHeader.mergeDistinct(origHDU.getHeader());

  // 3. Update the WCS for the cropped data...
  newHeader.addValue(Standard.CRPIXn.n(1), ...);
  ...
  
  // 4. Update checksums, if necessary
  newHDU.setChecksum();
</code></pre><hr />
<p></p></section><section><a id="Creating_tables"></a>
<h2>Creating tables</h2>
<ul>

<li><a href="#building-by-row">Building tables row-by-row</a></li>
<li><a href="#building-by-column">Building tables column-by-column</a></li>
<li><a href="#creating-ascii-tables">Creating ASCII tables (discouraged)</a></li>
</ul>
<p></p><section><a id="Building_tables_row-by-row"></a>
<h3>Building tables row-by-row</h3>
<p>As of version <strong>1.18</strong> building tables one row at a time is both easy and efficient &#x2013; and may be the least confusing
way to get tables done right. (In prior releases, adding rows to existing tables was painfully slow, and much more
constrained).</p>
<p>You may want to start by defining the types and dimensions of the data (or whether variable-length) that will be
contained in each table column:</p>

<pre class="prettyprint"><code class="language-java">   BinaryTable table = new BinaryTable();
   
   // A column containing 64-bit floating point scalar values, 1 per row...
   table.addColumn(ColumnDesc.createForScalars(double.class));
   
   // A column containing 5x4 arrays of single-precision complex values...
   table.addColumn(ColumnDesc.createForArrays(ComplexValue.Float.class, 5, 4));
   
   // A column containing Strings of variable length using 32-bit heap pointers...
   table.addColumn(ColumnDesc.createForVariableLength(String.class));
   
   ...
</code></pre>
<p>Defining columns this way is not always necessary before adding rows to the table. However, it is necessary if you
will have data that needs variable-length storage row-after-row; or if you want more control over specifics of the
column format. As such, it is best practice to define the columns explicitly even if not strictly required for your
particular application.</p>
<p>Now you can populate the table with your data, one row at a time, using the <code>addRow()</code> method as many times over as
necessary:</p>

<pre class="prettyprint"><code class="language-java">   for (...) {
       // prepare the row data, making sure each row is compatible with prior rows...
       ...
   	
       // Add the row to the table
       table.addRow(...);
   }
</code></pre>
<p>As of version <strong>1.18</strong>, you may use Java boxed types (as an alternative to primitive arrays-of-one) to specify
primitive scalar table elements, including auto-boxing of literals or variables. You may also use <em>vararg</em> syntax for
adding rows if that is more convenient in your application. Thus, you may simply write:</p>

<pre class="prettyprint"><code class="language-java">   table.addRowEntries(1, 3.14159265);
</code></pre>
<p>to add a row consisting of an 32-bit integer, a double-precision floating point value (presuming your table has those
two types of columns). Prior to <strong>1.18</strong>, the same would have to have been written as:</p>

<pre class="prettyprint"><code class="language-java">  table.addRow(new Object[] { new int[] {1}, new double[] {3.14159265} }; 
</code></pre>
<p>Tables built entirely row-by-row are naturally defragmented, as long as they are not modified subsequently.</p>
<p>Once the table is complete, you can make a HDU from it:</p>

<pre class="prettyprint"><code class="language-java">  BinaryTableHDU hdu = table.toHDU();
</code></pre>
<p>which will populate the header with the requisite entries that describe the table. You can then edit the new header
to add any extra information (while being careful to not modify the essential table description). Note, that once the
table is encompassed in an HDU, it is generally not safe to edit the table data, since the library has no foolproof
way to keep the header description of the table perfectly in sync. Thus it is recommended that you create table HDUs
only after the table data has been fully populated.</p>
<p>A few rules to remember when building tables by rows:</p>
<ul>

<li>All rows must contain the same number of entries (the number of columns)</li>
<li>Entries in the same column must match in their column type in every row.</li>
<li>Entries must be of the following supported types:
<ul>

<li>A supported Java type (<code>String</code> or <code>ComplexValue</code>), or</li>
<li>primitive arrays (e.g. <code>int[]</code>, <code>float[][]</code>), or</li>
<li>Arrays of <code>Boolean</code> (logicals), <code>String</code> or <code>ComplexValue</code> (such as <code>Boolean[][]</code> or <code>String[]</code>), or</li>
<li>Scalar primitives stored as arrays of 1 (e.g. <code>short[1]</code>).</li>
</ul></li>
<li>If entries are multi-dimensional arrays, they must have the same dimensionality and shape in every row.
(Otherwise, they will be stored as variable-length arrays in flattened 1D format, where the shape may be lost).</li>
<li>If entries are one-dimensional, they can vary in size from row to row freely.</li>
<li>Java <code>null</code> entries are allowed for <code>String</code> and <code>Boolean</code> (logical) types, but not for the other data types.
(these will map to empty strings or <em>undefined</em> logical values respectively)</li>
</ul>
<p></p></section><section><a id="Building_tables_column-by-column"></a>
<h3>Building tables column-by-column</h3>
<p>Sometimes we might want to assemble a table from a selection of data which will readily constitute columns in the table.
We can add these as columns to an existing table (empty or not) using the <code>BinaryTable.addColumn(Object)</code> method.
For example, say we have two arrays, one a time-series of spectra, and a matching array of corresponding timestamps. We
can create a table with these (or add them to an existing table with a matching number of rows) as:</p>

<pre class="prettyprint"><code class="language-java">   double[] timestamps = new double[nRows]; 
   ComplexValue[][] spectra = new ComplexValue[nRows][];
   ...
   
   BinaryTable tab = new BinaryTable();
   
   table.addColumn(timestamps);
   table.addColumn(spectra);
</code></pre>
<p>There are just a few rules to keep in mind when constructing tables in this way:</p>
<ul>

<li>All columns added this way must contain the same number of elements (number of rows).</li>
<li>In column data, scalars entries are simply elements in a 1D primitive array (e.g. <code>double[]</code>), in which each
element (e.g. a <code>double</code>) is the scalar value for a given row. (I.e. unlike in the row-major table format required
to create entire tables at once, we do not have to wrap scalar values in self-contained arrays of 1)</li>
<li>Other than the above, the same rules apply as for creating HDUs row-by-row (above).</li>
<li>If setting complex columns with arrays of <code>float[2]</code> or <code>double[2]</code> (the old way), you will want to call
<code>setComplexColumn(int)</code> afterwards for that column to make sure they are labeled properly in the FITS header
(rather than as real-valued arrays of <code>float</code> or <code>double</code>).</li>
<li>Similarly, if adding arrays of <code>boolean</code> values, you might consider calling <code>convertToBits(int)</code> on that
column for a more compact storage option of the <code>true</code>/<code>false</code> values, rather than as 1-byte FITS logicals
(default).</li>
</ul>
<p>Defragmenting might also be a good idea before writing binary tables with variable-length data built column by column
(as opposed to row-by-row):</p>

<pre class="prettyprint"><code class="language-java">  table.defragment();
</code></pre>
<p>before calling <code>write()</code> on the encompassing HDU.</p>
<p></p></section><section><a id="Creating_ASCII_tables_.28discouraged.29"></a>
<h3>Creating ASCII tables (discouraged)</h3>
<p>While the library also supports ASCII tables for storing a more limited assortment of <em>scalar</em> entries, binary tables
should always be your choice for storing table data. ASCII tables are far less capable overall. And while they may be
readable from a console without the need for other tools, there is no compelling reason for using ASCII tables
today. Binary tables are simply better, because they:</p>
<ul>

<li>Support arrays (including multidimensional and variable-length).</li>
<li>Support more data types (such as logical, and complex values).</li>
<li>Offer additional flexibility, such as variable sized and multi-dimensional array entries.</li>
<li>Take up less space on disk</li>
<li>Can be compressed to an even smaller size</li>
</ul>
<p>However, if you insist on creating ASCII tables (provided the data allows for it) you may:</p>
<ul>

<li>Build them column by column using one of the <code>AsciiTable.addColumn(...)</code> method, or</li>
<li>Build all at once, from a set of readily available columns via <code>AsciiTable.fromColumnMajor(Object[])</code>
(since <strong>1.19</strong>), or else</li>
<li>Set <code>FitsFactory.setUseAsciiTables(true)</code> prior to calling  <code>Fits.makeHDU()</code> or one of the factory methods to
encapsulate a column-major table data objects automatically as ASCII tables whenever it is possible.</li>
</ul>
<p>(Note that while the <code>AsciiTable</code> class also provides an <code>.addRow(Object[])</code> method, we strongly recommend against
it because it is extremely inefficient, i.e. painfully slow). Either way, you should keep in mind the inherent
limitations of ASCII tables:</p>
<ul>

<li>Only scalar entries are allowed (no arrays whatsoever!)</li>
<li>Only <code>int</code>, <code>long</code>, <code>float</code>, <code>double</code> and <code>String</code> entries are supported.</li>
</ul><hr />
<p></p></section></section><section><a id="Compression_support"></a>
<h2>Compression support</h2>
<ul>

<li><a href="#file-compression">File level compression</a></li>
<li><a href="#image-compression">Image compression</a></li>
<li><a href="#table-compression">Table compression</a></li>
</ul>
<p>Starting with version <strong>1.15</strong> we include support for compressing images and tables. The compression algorithms have
been ported to Java from <strong>cfitsio</strong> to provide a pure 100% Java implementation. However, versions prior to <strong>1.19.1</strong>
had a number of lingering compression related bugs of varying severity, which may have prevented reliable use.</p>
<p></p><section><a id="File_level_compression"></a>
<h3>File level compression</h3>
<p>It is common practice to compress FITS files using <strong>gzip</strong> (<code>.gz</code> extension) so they can be exchanged in a more
compact form. Java 8+ supports the creation of gzipped FITS out of the box, by wrapping the file's output
stream into a <code>GZIPOutputStream</code> or , such as:</p>

<pre class="prettyprint"><code class="language-java">  Fits f = ...
  
  FitsOutputStream out = new FitsOutputStream(new GZIPOutputStream(
  	new FileOutputStream(new File(&quot;mydata.fits.gz&quot;))));
  f.write(out);
</code></pre>
<p>While we only support GZIP compression for writing compressed files (thanks to Java's support out of the box), we can
read more compressed formats using Apaches commons-compress library. We support reading compressed files produced via
<strong>gzip</strong> (<code>.gz</code>), the Linux <strong>compress</strong> tools (<code>.Z</code>), and via <strong>bzip2</strong> (<code>.bz2</code>). The decompression happens
automatically when we construct a <code>Fits</code> object with an input stream:</p>

<pre class="prettyprint"><code class="language-java">  new FileInputStream compressedStream = new FileInputStream(new File(&quot;image.fits.bz2&quot;));
 
  // The input stream will be filtered through a decompression algorithm
  // All read access to the FITS will pass through that decompression...
  Fits fits = new Fits(compressedStream);
  ...
</code></pre>
<p></p></section><section><a id="Image_compression"></a>
<h3>Image compression</h3>
<p>Image compression and tiling are fully supported by <strong>nom.tam.fits</strong> as of <strong>1.18</strong>, including images of
any dimensionality and rectangular morphologies. (Releases between <strong>1.15</strong> and <strong>1.17</strong> had partial image
compression support for 2D square images only, while some quantization support for compression was
lacking prior to <strong>1.18</strong>).</p>
<p>The tiling of non-2D images follows the
<a href="https://heasarc.gsfc.nasa.gov/docs/software/fitsio/compression.html" class="externalLink">CFITSIO convention</a> with 2D tiles,
where the tile size is set to 1 in the higher dimensions.</p>
<p>Compressing an image HDU is typically a multi-step process:</p>
<ol style="list-style-type: decimal;">

<li>Create a <code>CompressedImageHDU</code>, e.g. with <code>fromImageHDU(ImageHDU, int...)</code>:</li>
</ol>

<pre class="prettyprint"><code class="language-java">  ImageHDU image = ...
 
  CompressedImageHDU compressed = CompressedImageHDU.fromImageHDU(image, 60, 40);
</code></pre>
<ol style="list-style-type: decimal;">

<li>Set up the compression algorithm, including quantization (if desired) via <code>setCompressAlgorithm(String)</code> and
<code>setQuantAlgorithm(String)</code>, and optionally the compression method used for preserving the blank values via
<code>preserveNulls(String)</code>:</li>
</ol>

<pre class="prettyprint"><code class="language-java">  compressed.setCompressAlgorithm(Compression.ZCMPTYPE_RICE_1)
            .setQuantAlgorithm(Compression.ZQUANTIZ_SUBTRACTIVE_DITHER_1)
            .preserveNulls(Compression.ZCMPTYPE_HCOMPRESS_1);
</code></pre>
<ol style="list-style-type: decimal;">

<li>Set compression (and quantization) options, via calling on <code>getCompressOption(Class)</code>:</li>
</ol>

<pre class="prettyprint"><code class="language-java">  compressed.getCompressOption(RiceCompressOption.class).setBlockSize(32);
  compressed.getCompressOption(QuantizeOption.class).setBZero(3.0).setBScale(0.1).setBNull(-999);
</code></pre>
<ol style="list-style-type: decimal;">

<li>Finally, perform the actual compression via <code>compress()</code>:</li>
</ol>

<pre class="prettyprint"><code class="language-java">  compressed.compress(); 
</code></pre>
<p>After the compression, the compressed image HDU can be handled just like any other HDU, and written to a file or
stream, for example (just not as the first HDU in a FITS&#x2026;).</p>
<p>The reverse process is simply via the <code>asImageHDU()</code> method. E.g.:</p>

<pre class="prettyprint"><code class="language-java">  CompressedImageHDU compressed = ...
  ImageHDU image = compressed.asImageHDU();
</code></pre>
<p>When compressing or decompression images, all available CPUs are automatically utilized.</p><section><a id="Accessing_image_header_values_without_decompressing.3A"></a>
<h4>Accessing image header values without decompressing:</h4>
<p>You don't need to decompress the image to see what the decompressed image header is. You can simply call
<code>CompressedImageHDU.getImageHeader()</code> to peek into the reconstructed header of the image before it was compressed:</p>

<pre class="prettyprint"><code class="language-java">  CompressedImageHDU compressed = ...
  Header imageHeader = compressed.getImageHeader();
</code></pre></section><section><a id="Accessing_specific_parts_of_a_compressed_image"></a>
<h4>Accessing specific parts of a compressed image</h4>
<p>Often compressed images can be very large, and we are interested in specific areas of it only. As such, we do not want
to decompress the entire image. In these cases we can use the <code>getTileHDU()</code> method of <code>CompressedImageHDU</code> class to
decompress only the selected image area. As of version <strong>1.18</strong>, this is really easy also:</p>

<pre class="prettyprint"><code class="language-java">  CompressedImageHDU compressed = ...
   
  int[] fromPixels = ...
  int[] cutoutSize = ...
   
  ImageHDU cutout = compressed.getTileHDU(fromPixels, cutoutSize);
</code></pre>
<p></p></section></section><section><a id="Table_compression"></a>
<h3>Table compression</h3>
<p>Table compression is also supported in <strong>nom.tam.fits</strong> from version <strong>1.15</strong>, and more completely since <strong>1.19.1</strong>.
When compressing a table, the &#x2018;tiles&#x2019; are sets of contiguous rows within a column. Each column may use a different
compression algorithm. As for FITS version 4.0 only lossless compression is supported for tables, and hence only
<code>GZIP_1</code>, <code>GZIP_2</code> (default), <code>RICE_1</code> (with default options), and <code>NOCOMPRESS</code> are admissible. The default
compression is with <code>GZIP_2</code>, unless explicitly defined otherwise.</p>
<p>Tile compression mimics image compression, and is typically a 2-step process:</p>
<ol style="list-style-type: decimal;">

<li>Create a <code>CompressedTableHDU</code>, e.g. with <code>fromBinaryTableHDU(BinaryTableHDU, int, String...)</code>, using the
specified number of table rows per compressed block, and compression algorithm(s), e.g. <code>RICE_1</code> for the
first column, and <code>GZIP_2</code> for the rest:</li>
</ol>

<pre class="prettyprint"><code class="language-java">   BinaryTableHDU table = ...
   CompressedTableHDU compressed = CompressedTableHDU.fromBinaryTableHDU(table, 4, Compression.RICE_1);
</code></pre>
<ol style="list-style-type: decimal;">

<li>Perform the compression via <code>compress()</code>:</li>
</ol>

<pre class="prettyprint"><code class="language-java">   compressed.compress();
</code></pre>
<p>The two step process (as opposed to a single-step one) was probably chosen because it mimics that of
<code>CompressedImageHDU</code>, where further configuration steps may be inserted in-between (which might become possible in a
future FITS standard). But, of course we can combine the steps into a single line:</p>

<pre class="prettyprint"><code class="language-java">   CompressedTableHDU compressed = CompressedTableHDU.fromBinaryTableHDU(table, 4, Compression.RICE_1).compress();
</code></pre>
<p>After the compression, the compressed table HDU can be handled just like any other HDU, and written to a file or
stream, for example.</p>
<p>The reverse process is simply via the <code>asBinaryTableHDU()</code> method. E.g.:</p>

<pre class="prettyprint"><code class="language-java">    CompressedTableHDU compressed = ...
    BinaryTableHDU table = compressed.asBinaryTableHDU();
</code></pre><section><a id="Accessing_image_header_values_without_decompressing"></a>
<h4>Accessing image header values without decompressing</h4>
<p>You don't need to decompress the table to see what the decompressed table header is. You can simply call
<code>CompressedTableHDU.getTableHeader()</code> to peek into the reconstructed header of the original table before it was
compressed:</p>

<pre class="prettyprint"><code class="language-java">   CompressedTableHDU compressed = ...
   Header origHeader = compressed.getTableHeader();
</code></pre></section><section><a id="Decompressing_select_parts_of_a_compressed_binary_table"></a>
<h4>Decompressing select parts of a compressed binary table</h4>
<p>Sometimes we are interested in a section of the compressed table only. As of version <strong>1.18</strong>, this is really easy
also. If you just want to uncompress a range of the compressed tiles, you can</p>

<pre class="prettyprint"><code class="language-java">   CompressedTableHDU compressed = ...
   TableHDU section = compressed.asTableHDU(fromTile, toTile);
</code></pre>
<p>The resulting HDU will contain all columns but on only the uncompressed rows for the selected tiles.</p>
<p>And, if you want to surgically access a range of data from select columns (and tiles) only:</p>

<pre class="prettyprint"><code class="language-java">   CompressedTableHDU compressed = ...
   Object[] colData = compressed.getColumnData(colIndex, fromTile, toTile);
</code></pre>
<p>The methods <code>CompressedTableHDU.getTileRows()</code> and <code>.getTileCount()</code> can be used to help determined which tile(s)
to decompress to get access to specific table rows.</p></section><section><a id="Note_on_compressing_variable-length_arrays_.28VLAs.29"></a>
<h4>Note on compressing variable-length arrays (VLAs)</h4>
<p>The compression of variable-length table columns is a fair bit more involved process than that for fixed-sized table
entries, and we only started properly supporting it in <strong>1.19.1</strong>. When compressing/decompressing tables containing
VLAs, you should be aware of the very limited interoperability with other tools, including (C)FITSIO and its <code>fpack</code> /
<code>funpack</code> (more on these below). In fact, we are not aware of any tool other than <strong>nom.tam.fits</strong> that offers a
truly complete and accurate implementation of this part of the standard.</p>
<p>Note, that the <a href="https://heasarc.gsfc.nasa.gov/fitsio/" class="externalLink">(C)FITSIO</a> implementation of VLA compression diverges from the
documented standard (FITS 4.0 and the original Pence et al. 2013 convention) by storing the adjoined descriptors in
reversed order, w.r.t. the standard, on the heap. Our understanding, based on communication with the maintainers of
the FITS standard, is that this discrepancy will be resolved by changing the documentation (the standard) to conform to
the (C)FITSIO implementation. Therefore, our implementation for the compression of VLAs is generally compliant to that
of (C)FITSIO, and not to the current prescription of the standard. However, we wish to support reading files produced
either way via the <code>static</code> <code>CompressedTableHDU.useOldStandardVLAIndexing(boolean)</code> method selecting the convention
according to which the adjoined table descriptors are stored in the file: either in the format described by the
original FITS 4.0 standard / Pence+2013 convention (<code>true</code>), or else in the (C)FITSIO compatible format (<code>false</code>;
default).</p></section><section><a id="Interoperability_with_.28C.29FITSIO.27s_fpack_.2F_funpack"></a>
<h4>Interoperability with (C)FITSIO's <code>fpack</code> / <code>funpack</code></h4>
<p>The table compression implementation of <strong>nom.tam.fits</strong> is now both more standard(!) and  more reliable(!) than that
of (C)FITSIO and <code>fpack</code> / <code>funpack</code>. Issues of interoperability are not due to a fault of our own. Specifically,
the current (4.5.0) and earlier <a href="https://heasarc.gsfc.nasa.gov/fitsio/" class="externalLink">(C)FITSIO</a> releases are affected by a slew of
table-compression related bugs, quirks, and oddities &#x2013; which severely limit its interoperability with other tools.
Some of the bugs in <code>fpack</code> may result in entirely corrupted FITS files, while others limit what standard compressed
data <code>funpack</code> is able to decompress:</p>
<ol style="list-style-type: decimal;">

<li>
<p>(C)FITSIO and <code>fpack</code>  version &lt;= 4.5.0 do not properly handle the <code>THEAP</code> keyword (if present). If the keyword
is present, <code>fpack</code> will use it incorrectly, resulting in a bloated compressed FITS that is also unreadable because of
an incorrect <code>PCOUNT</code> value in the compressed header. Therefore, we will skip adding <code>THEAP</code> to the table headers when
not necessary (that is when the heap follows immediately after the main table), in order to provide better
interoperability with (C)FITSIO and <code>fpack</code>.</p></li>
<li>
<p>(C)FITSIO and <code>fpack</code> version &lt;= 4.5.0 do not handle <code>byte</code>-type and <code>short</code>-type VLA columns properly. In the
<code>fpack</code>-compressed headers these are invariably indicated as compressed via <code>GZIP_1</code> and <code>GZIP_2</code> respectively
(regardless of the user-specified compression option), whereas the data on the heap is not actually compressed
for either. And while <code>fpack</code> does compress <code>int</code> type VLAs properly, it does it with <code>RICE_1</code> always, regardless of
the user selection for the compression option. To a lesser extent, the <code>fpack</code> compression of fixed-sized columns
is also lacking: fixed-sized <code>byte[]</code> and/or <code>short[]</code> array columns they are invariably compressed with <code>GZIP_1</code> and
<code>GZIP_2</code>, respectively, as indicated (but still ignoring the user's choice for the requested compression type).</p></li>
<li>
<p>(C)FITSIO and <code>funpack</code> version &lt;= 4.5.0 do not handle uncompressed data columns (with <code>ZCTYPn = 'NOCOMPRESS')</code>
in compressed tables, despite these being standard.</p></li>
</ol><hr />
<p></p></section></section></section><section><a id="Release_schedule"></a>
<h2>Release schedule</h2>
<p>A predictable release schedule and process can help manage expectations and reduce stress on adopters and developers
alike.</p>
<p>Releases of the library follow a quarterly release schedule since version <strong>1.16</strong>. You may expect upcoming releases
to be published around <strong>March 15</strong>, <strong>June 15</strong>, <strong>September 15</strong>, and/or <strong>December 15</strong> each year, on an as needed
basis. That means that if there are outstanding bugs, or new pull requests (PRs), you may expect a release that
addresses these in the upcoming quarter. The dates are placeholders only, with no guarantee that a release will
actually be available every quarter. If nothing of note comes up, a potential release date may pass without a release
being published.</p>
<p><em>Feature releases</em> (<strong>1.x.0</strong> version bumps), which may include significant API changes, are provided at least 6
months apart, to reduce stress on adopters who may need/want to tweak their code to integrate these. Between feature
releases, <em>bug fix releases</em> (without significant API changes) may be provided as needed to address issues. New
features are generally reserved for the feature releases, although they may also be rolled out in bug-fix releases as
long as they do not affect the existing API &#x2013; in line with the desire to keep bug-fix releases fully backwards
compatible with their parent versions.</p>
<p>In the month(s) preceding releases one or more <em>release candidates</em> (e.g. <code>1.19.1-rc3</code>) will be available on GitHub
briefly, under <a href="https://github.com/nom-tam-fits/nom-tam-fits/releases" class="externalLink">Releases</a>, so that changes can be tested by
adopters before the releases are finalized. Please use due diligence to test such release candidates with your code
when they become available to avoid unexpected surprises when the finalized release is published. Release candidates
are typically available for one week only before they are superseded either by another, or by the finalized release.</p><hr />
<p></p></section><section><a id="How_to_contribute"></a>
<h2>How to contribute</h2>
<p>The <em>nom-tam-fits</em> library is a community-maintained project. We absolutely rely on developers like you to make it
better and to keep it going. Whether there is a nagging issue you would like to fix, or a new feature you'd like to
see, you can make a difference yourself. We welcome you as a contributor. You became part of our community the moment
you landed on this page. We very much encourage you to make this project a little bit your own, by submitting pull
requests with fixes and enhancement. When you are ready, here are the typical steps for
contributing to the project:</p>
<ol style="list-style-type: decimal;">

<li>
<p>Old or new <strong>Issue</strong>? Whether you just found a bug, or you are missing a much needed feature, start by checking
open (and closed) <a href="https://github.com/nom-tam-fits/nom-tam-fits/issues" class="externalLink">Issues</a>. If an existing issue seems like a
good match to yours, feel free to comment on it, and/or to offer help in resolving it. If you find no issues that
match, go ahead and create a new one.</p></li>
<li>
<p><strong>Fork</strong>. Is it something you'd like to help resolve? Great! You should start by creating your own fork of the
repository so you can work freely on your solution. We also recommend that you place your work on a branch of your
fork, which is named either after the issue number, e.g. <code>issue-192</code>, or some other descriptive name, such as
<code>implement-foreign-hdu</code>.</p></li>
<li>
<p><strong>Develop</strong>. Feel free to experiment on your fork/branch. If you run into a dead-end, you can always abandon it
(which is why branches are great) and start anew. You can run your own test builds locally using <code>mvn clean test</code>
before committing your changes. If the tests pass, you should also try running <code>mvn clean package</code> and
<code>mvn site stage</code> to ensure that the package and Javadoc are also in order. Remember to synchronize your <code>master</code>
branch by fetching changes from upstream every once in a while, and rebasing your development branch. Don't
forget to:</p>
<ul>

<li>
<p>Add <strong>Javadoc</strong> your new code. You can keep it sweet and simple, but make sure it properly explains your methods,
their arguments and return values, and why and what exceptions may be thrown. You should also cross-reference other
methods that are similar, related, or relevant to what you just added.</p></li>
<li>
<p>Add <strong>Unit Tests</strong>. Make sure your new code has as close to full unit test coverage as possible. You should aim
for 100% diff coverage. When pushing changes to your fork, you can get a coverage report by checking the GitHub
Actions result of your commit (click the Codecov link), and you can analyze what line(s) of code need to have tests
added. Try to create tests that are simple but meaningful (i.e. check for valid results, rather than just confirm
existing behavior), and try to cover as many realistic scenarios as appropriate. Write lots of tests if you need to.
It's OK to write 100 lines of test code for 5 lines of change. Go for it! And, you will get extra kudos for filling
unit testing holes outside of your area of development!</p></li>
</ul></li>
<li>
<p><strong>Pull Request</strong>. Once you feel your work can be integrated, create a pull request from your fork/branch. You can
do that easily from the GitHub page of your fork/branch directly. In the pull request, provide a concise description
of what you added or changed. Your pull request will be reviewed. You may get some feedback at this point, and maybe
there will be discussions about possible improvements or regressions etc. It's a good thing too, and your changes will
likely end up with added polish as a result. You can be all the more proud of it in the end!</p></li>
<li>
<p>If all goes well, your pull-request will get merged, and will be included in the upcoming release of
<em>nom-tam-fits</em>. Congratulations for your excellent work, and many thanks for dedicating some of your time for making
this library a little bit better. There will be many who will appreciate it. :-)</p></li>
</ol>
<p>If at any point you have questions, or need feedback, don't be afraid to ask. You can put your questions into the
issue you found or created, or your pull-request, or as a Q&amp;A in
<a href="https://github.com/nom-tam-fits/nom-tam-fits/discussions" class="externalLink">Discussions</a>.</p></section></section>        </main>
      </div>
    </div>
    <hr/>
    <footer>
      <div class="container-fluid">
        <div class="row-fluid">
nom.tam.fits documentation
        </div>
      </div>
    </footer>
  </body>
</html>
